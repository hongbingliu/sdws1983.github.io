<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[年度巨作！The big data of The Plant Cell : PC年度大数据抓取及分析(一)]]></title>
      <url>http://yoursite.com/2017/01/09/%E5%B9%B4%E5%BA%A6%E5%B7%A8%E4%BD%9C%EF%BC%81The-big-data-of-The-Plant-Cell-PC%E5%B9%B4%E5%BA%A6%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%93%E5%8F%96%E5%8F%8A%E5%88%86%E6%9E%90-%E4%B8%80/</url>
      <content type="html"><![CDATA[<p>只是突然冒出这么一个想法，在读文献的时候。<br>好久没写爬虫了，顺便还可以练练R。</p>
<p><em>The Plant Cell</em>，1989年创刊，月刊，5年<em>IF</em>：10.529。<br>毫无疑问是植物学最顶级的杂志，植物人的梦想。<br><a id="more"></a></p>
<p>我大概的目标是抓取每一篇研究文章的摘要，顺带通讯作者和作者所在机构，之后看看用分词的包把摘要进行分词，最后计算词频，以及作者、机构的频数。</p>
<p>首先进入PC网站，随便找一篇文章进入它的摘要界面：<br><img src="http://p1.bqimg.com/567571/5f7f6cf8fcbabc52.jpg" alt=""></p>
<p>右键查看网页源代码，OK，可以爬取，顺带连作者机构都在一个页面，一起抓了。<br>再观察网页地址，发现<em>28/11/2715</em>，28为年份数，2016年的刊物都为28，11为月份数，代表11月，2715则为页码。</p>
<p>下图为2016年PC的月刊详情：<br><img src="http://p1.bqimg.com/567571/01ef43a484945aa1.jpg" alt=""></p>
<p>所以现在要从哪里获得这些文章的页码信息呢，发现在每一月期刊下有一个TOC：<br><img src="http://p1.bqimg.com/567571/bbbdb315b598ed56.jpg" alt=""></p>
<p>TOC就是table of content，点开一看是个PDF，就相当于目录的意思。<br><img src="http://p1.bqimg.com/567571/a9874f8f8cfa59d7.jpg" alt=""></p>
<p>目录上就有页码，似乎只要把这些页码爬下来就可以了。<br>但由于这个TOC是PDF，把它爬下来之后还不能直接解析，所以我又找了一个解析PDF文件的包，pdfminer，直接开搞！<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> urllib2 <span class="keyword">import</span> urlopen</div><div class="line"><span class="keyword">from</span> pdfminer.pdfinterp <span class="keyword">import</span> PDFResourceManager, PDFPageInterpreter</div><div class="line"><span class="keyword">from</span> pdfminer.converter <span class="keyword">import</span> TextConverter</div><div class="line"><span class="keyword">from</span> pdfminer.layout <span class="keyword">import</span> LAParams</div><div class="line"><span class="keyword">from</span> pdfminer.pdfpage <span class="keyword">import</span> PDFPage</div><div class="line"><span class="keyword">from</span> cStringIO <span class="keyword">import</span> StringIO</div><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_pdf_to_txt</span><span class="params">(fp)</span>:</span></div><div class="line">    rsrcmgr = PDFResourceManager()</div><div class="line">    retstr = StringIO()</div><div class="line">    codec = <span class="string">'utf-8'</span></div><div class="line">    laparams = LAParams()</div><div class="line">    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)</div><div class="line">    interpreter = PDFPageInterpreter(rsrcmgr, device)</div><div class="line">    password = <span class="string">""</span></div><div class="line">    maxpages = <span class="number">0</span></div><div class="line">    caching = <span class="keyword">True</span></div><div class="line">    pagenos=set()</div><div class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=<span class="keyword">True</span>):</div><div class="line">        interpreter.process_page(page)</div><div class="line"></div><div class="line">    fp.close()</div><div class="line">    device.close()</div><div class="line">    textstr = retstr.getvalue()</div><div class="line">    retstr.close()</div><div class="line">    <span class="keyword">return</span> textstr</div></pre></td></tr></table></figure></p>
<p>因为是在实验室服务器上写的，环境是Python2.7。从网上参考的资料，把函数写好，最后返回的是解析的pdf，截取一小段解析的结果：<br><img src="http://p1.bqimg.com/567571/06d58bfba073b5ce.jpg" alt=""></p>
<p>效果不错，发现页码都是单独成行，这样就非常利于提取了。</p>
<p>再把页码提取出来，最后把一年的期刊都爬取一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_issues</span><span class="params">(iss)</span>:</span></div><div class="line">   </div><div class="line">   url=<span class="string">'http://www.plantcell.org/content/28/'</span> + str(iss) + <span class="string">'.toc.pdf'</span></div><div class="line">   fp = StringIO(urlopen(url).read())</div><div class="line">   text=convert_pdf_to_txt(fp)</div><div class="line">   text = str(text).split(<span class="string">"\n"</span>)</div><div class="line">   all = []</div><div class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> text:</div><div class="line">      <span class="comment">#print (i)</span></div><div class="line">      <span class="keyword">if</span> re.findall(<span class="string">'\D+'</span>, i):</div><div class="line">         <span class="keyword">pass</span></div><div class="line">      <span class="keyword">elif</span> i != <span class="string">''</span> :</div><div class="line">         all.append(i)</div><div class="line">   <span class="keyword">print</span> (all)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">   <span class="keyword">for</span> iss <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">12</span>):</div><div class="line">      <span class="comment">#print (iss)</span></div><div class="line">      get_issues(iss)</div></pre></td></tr></table></figure></p>
<p>split按回车分割text，再把text遍历，找出没有非数字出现的文本，并去除空字符，最后把每个页码添加进all列表，输出即可。</p>
<p>结果：<br><img src="http://p1.bqimg.com/567571/b548236b2940a0fc.jpg" alt=""></p>
<p>成功完成，这个获得页码的脚本就写好了，留在一边备用。</p>
<p>现在来解析文章的摘要等信息，回到摘要网页，查看源代码。<br>很简单的使用<em>BeautifulSoup4</em>把信息提取出来，过程就不赘述。<br><img src="http://p1.bqimg.com/567571/ef51c7b49658debb.jpg" alt=""></p>
<p>代码太多，不贴了，放在<a href="https://github.com/sdws1983/The-big-data-OF-The-Plant-Cell" target="_blank" rel="external">github</a>上。<br>花了半天的时间，今天就先做到这了，之后有空再继续。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[用微博实现远程关机]]></title>
      <url>http://yoursite.com/2016/10/01/%E7%94%A8%E5%BE%AE%E5%8D%9A%E5%AE%9E%E7%8E%B0%E8%BF%9C%E7%A8%8B%E5%85%B3%E6%9C%BA/</url>
      <content type="html"><![CDATA[<p>平时工作日电脑都是放在实验室，很少带回去，有的时候还要运行一些东西，人先回宿舍了，电脑也跟着熬了一宿夜。像我这种老破电脑，最怕逞强，对其损伤还是很大的。<br>前些天看了一些不错的思路，基本差不多，但他使用了微博的API，我觉得有点麻烦，于是换了自己的方法。<br><a id="more"></a></p>
<hr>
<h1 id="Thought"><a href="#Thought" class="headerlink" title="Thought"></a>Thought</h1><p>首先通过读取最新一条微博内容获取指令：设置一个关键词，当检测到微博出现关键词，即可触发关机指令；同时考虑只有新微博的指令才有效，进行一些逻辑判断。<br>关机是否顺利呢？这是在用到发送邮件的模块，成功关机则发送邮件到指定邮箱。<br>大致如此。</p>
<hr>
<h1 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h1><p>先来写获取微博内容的部分：<br>因为移动版微博的内容较少，速度也快点，所以抓的移动版微博，找到自己主页个人微博的地址。<br><em>headers</em>伪装，之后再使用<em>BeautifulSoup</em>及<em>find</em>进行内容提取，比较简单。<br><em>json.loads</em>转化为字典，之后提取更加方便。<br>最后返回最新一条微博的发布时间<em>cr_time</em>，微博id号<em>id_</em>及微博内容<em>text</em>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weibo_content</span><span class="params">()</span>:</span></div><div class="line">	url = <span class="string">"http://m.weibo.cn/page/tpl?containerid=XXXXXXXXXXXXXXX_-_WEIBO_SECOND_PROFILE_WEIBO"</span></div><div class="line">	send_headers = &#123;</div><div class="line">		<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.80 Safari/537.36'</span>,</div><div class="line">		<span class="string">'Accept'</span>: <span class="string">'*/*'</span>,</div><div class="line">		<span class="string">'Connection'</span>: <span class="string">'keep-alive'</span>,</div><div class="line">		<span class="string">'Host'</span>: <span class="string">'m.weibo.cn'</span>,</div><div class="line">    	&#125;</div><div class="line"></div><div class="line">	r = requests.get(url, headers=send_headers)</div><div class="line">	data = r.text</div><div class="line">	<span class="comment">#print (data)</span></div><div class="line">	soup = BeautifulSoup(data,<span class="string">'lxml'</span>)</div><div class="line">	content = soup.find_all(<span class="string">'script'</span>)[<span class="number">1</span>].string</div><div class="line">	<span class="comment">#print (content)</span></div><div class="line">	st = content.find(<span class="string">'&#123;"card_type"'</span>)</div><div class="line">	en = content.find(<span class="string">'&#123;"card_type"'</span>,st+<span class="number">2</span>) <span class="number">-1</span></div><div class="line">	content = content[st:en]</div><div class="line">	<span class="comment">#print (content)</span></div><div class="line"></div><div class="line">	content = json.loads(content)</div><div class="line">	cr_time = content[<span class="string">'mblog'</span>][<span class="string">'created_at'</span>]</div><div class="line">	id_ = content[<span class="string">'mblog'</span>][<span class="string">'id'</span>]</div><div class="line">	text = content[<span class="string">'mblog'</span>][<span class="string">'text'</span>]</div><div class="line">	text = urllib.parse.unquote(text)</div><div class="line">	<span class="comment">#print (text)</span></div><div class="line">	<span class="keyword">return</span> cr_time, id_, text</div></pre></td></tr></table></figure></p>
<p>之后再写一下发送邮件的部分：<br><em>SMTP_host</em>为发送邮件地址的服务器，<em>from_addr</em>为发送邮件地址，<em>password</em>为发送邮件邮箱密码，<em>to_addrs</em>为接收邮件地址，<em>subject</em>为邮件主题，<em>content</em>为邮件内容。<br>比较简单。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> smtplib <span class="keyword">import</span> SMTP</div><div class="line"><span class="keyword">from</span> email.mime.text <span class="keyword">import</span> MIMEText</div><div class="line"><span class="keyword">from</span> email.header <span class="keyword">import</span> Header</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">send_email</span><span class="params">(SMTP_host, from_addr, password, to_addrs, subject, content)</span>:</span></div><div class="line">	email_client = SMTP(SMTP_host)</div><div class="line">	email_client.login(from_addr, password)</div><div class="line">	<span class="comment"># create msg</span></div><div class="line">	msg = MIMEText(content, <span class="string">'plain'</span>, <span class="string">'utf-8'</span>)</div><div class="line">	msg[<span class="string">'Subject'</span>] = Header(subject, <span class="string">'utf-8'</span>)<span class="comment">#subject</span></div><div class="line">	msg[<span class="string">'From'</span>] = from_addr</div><div class="line">	msg[<span class="string">'To'</span>] = to_addrs</div><div class="line">	email_client.sendmail(from_addr, to_addrs, msg.as_string())</div><div class="line"></div><div class="line">	email_client.quit()</div></pre></td></tr></table></figure>
<p>最后连接一下：<br>先获取之前的最新微博信息，进入循环，每次刷新再获得最新的微博信息，并进行比较，当最新微博id改变且新微博内容中有 <em>“shutdown”</em>这个关键词时，进行关机。<br>关机命令<em>os.system(‘shutdown -s -t 10’)</em>为10秒后关机，具体命令可再查询。<br><em>time.strftime(‘%H:%M:%S’)</em>为当前时间。<br>最后<em>sleep(20)</em>，表示每20秒执行一次。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">	cr_time, id_, text = get_weibo_content()</div><div class="line"></div><div class="line">	<span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">		cr_time_n, id_n, text_n = get_weibo_content()</div><div class="line">		<span class="keyword">print</span> (id_n)</div><div class="line">		<span class="keyword">print</span> (text_n)</div><div class="line"></div><div class="line">		<span class="keyword">if</span> str(id_n) != str(id_) <span class="keyword">and</span> (<span class="string">u'shutdown'</span> <span class="keyword">in</span> text_n):</div><div class="line">			mytime = <span class="string">'shutdown time : '</span> + str(time.strftime(<span class="string">'%H:%M:%S'</span>)) + <span class="string">". Process finished.."</span></div><div class="line">			send_email(<span class="string">"smtp.XXX.com"</span>, <span class="string">"YYYYYY@XXX.com"</span>, <span class="string">"YOUR PASSWORD"</span>, <span class="string">"ZZZZZZ@AAA.com"</span>, <span class="string">"您的电脑关机成功！"</span>, mytime)</div><div class="line">			os.system(<span class="string">'shutdown -s -t 10'</span>)</div><div class="line"></div><div class="line">			<span class="keyword">break</span></div><div class="line">		</div><div class="line">		sleep(<span class="number">20</span>)</div></pre></td></tr></table></figure>
<p>大体如此：-）</p>
<hr>
<h1 id="Showing"><a href="#Showing" class="headerlink" title="Showing"></a>Showing</h1><p>首先运行程序，发布微博，带有关键词。<br><img src="http://p1.bpimg.com/567571/df4f455451b151cf.png" alt=""></p>
<p>等一等，然后：<br><img src="http://p1.bpimg.com/567571/b6125fb7c8c9423a.png" alt=""><br>就是这么轻松：——）</p>
<hr>
<h1 id="Plus"><a href="#Plus" class="headerlink" title="Plus+"></a>Plus+</h1><p>这个自动发送邮件模块其实还是很好用的，如果你的邮箱有手机提醒的话就更完美了。<br>在此之前，我还应用这个邮件模块，写了一个自动查询火车票的脚本。</p>
<p><img src="http://i1.piimg.com/567571/2d373f6eb712b4a4.png" alt=""></p>
<p>通过它，我顺利的刷到了从某地回北京的硬卧票。。</p>
<p>怎么应用就看你了，希望你有一个不错的思路：——）</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[奥森阴天小游]]></title>
      <url>http://yoursite.com/2016/09/17/%E5%A5%A5%E6%A3%AE%E9%98%B4%E5%A4%A9%E5%B0%8F%E6%B8%B8/</url>
      <content type="html"><![CDATA[<p>中秋佳节，最后一日，与实验室友人游于奥森公园，天阴沉，并带相机小捏几张。<br><a id="more"></a></p>
<img src="/uploads/阴天头/Y3VyOUljK0lkdmhtcXdUMWVBV01iN01GUVByT1lDOFpkcmFzdVJGWWFPNitIZHg0YVkyUmZnPT0.jpg" class="full-image">
<img src="/uploads/阴天头/Y3VyOUljK0lkdmhtcXdUMWVBV01iOGEwSC9MMi9YWk5NbEMyMjgrRmNFdVRBcWZaQlArWHd3PT0.jpg" class="full-image">
<img src="/uploads/阴天头/Y3VyOUljK0lkdmhtcXdUMWVBV01iM1NGajFqaHVNb0hPYjR3M3FYaC9ScVFCbHQ5UXYzK0VnPT0.jpg" class="full-image">]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Min教你CAU一键上网]]></title>
      <url>http://yoursite.com/2016/09/15/CAU%E4%B8%80%E9%94%AE%E4%B8%8A%E7%BD%91/</url>
      <content type="html"><![CDATA[<p>到了新学校，搞不了什么大新闻，那至少也得完成点小目标。<br>一直觉得校园网这种登录方式浪费时间，打开浏览器，输入密码再跳转再输入，麻烦的不行。像哥这种日理万机的人，哪有时间干这种小事。于是前几周我花了一点下午的时间终于把这个自动登录的脚本写好并打包。<br>把整个思路记录梳理一下。<br><a id="more"></a></p>
<hr>
<h1 id="Simulation"><a href="#Simulation" class="headerlink" title="Simulation"></a>Simulation</h1><p>一般写爬虫的思路就是模拟浏览器的行为，cau的登录方式大致也还比较简单。</p>
<p>一打开浏览器，自动跳转至<a href="http://my.cau.edu.cn/" target="_blank" rel="external">my.cau.edu.cn</a>:<br><img src="http://i4.piimg.com/567571/27d7a95d2c735738.png" alt=""></p>
<p>之后点击左下角<strong>公共服务</strong>中的<strong>网络服务</strong>，进入了中国农业大学网络综合服务平台。<br><img src="http://i4.piimg.com/567571/491293e095d70eaa.png" alt=""></p>
<p>左边的网关登录，输入自己的账号密码即可。<br>登陆成功，显示网页：<br><img src="http://i4.piimg.com/567571/677731c53f9256f3.png" alt=""></p>
<p>整个过程基本如此。</p>
<hr>
<h1 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h1><p>发现登录这一步其实是向<em><a href="http://202.205.80.10/" target="_blank" rel="external">http://202.205.80.10/</a></em>post你的账号及密码：<br><img src="http://i4.piimg.com/567571/1258f66d21a4fc3c.png" alt=""><br>之后再get<em><a href="http://202.205.80.10/" target="_blank" rel="external">http://202.205.80.10/</a></em>的内容。<br>登录成功后网页有几个重要的信息，我们要获取它，比如名字，使用时间，使用流量等。</p>
<p>查看网页源代码，找到这几个信息的位置，获取它们即可。</p>
<hr>
<h1 id="Coding"><a href="#Coding" class="headerlink" title="Coding"></a>Coding</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</div><div class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</div></pre></td></tr></table></figure>
<p>导入需要的模块，这里用了<em>Requests</em>和<em>BeautifulSoup</em>，用来操控浏览器及网页信息获取。<br>为了使界面好看有意思一些，我还使用了一个进度条模块<em>tqdm</em>（虽然并没有必要）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">try</span>:</div><div class="line">	url = <span class="string">"http://202.205.80.10/"</span></div><div class="line">	send_headers = &#123;</div><div class="line">				<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.80 Safari/537.36'</span>,</div><div class="line">				<span class="string">'Accept'</span>:<span class="string">'*/*'</span>,</div><div class="line">				<span class="string">'Connection'</span>:<span class="string">'keep-alive'</span>,</div><div class="line">				<span class="string">'Host'</span>:<span class="string">'202.205.80.10'</span></div><div class="line">			&#125;</div><div class="line"></div><div class="line">	r = requests.post(url, headers = send_headers, data= &#123;<span class="string">'DDDDD'</span>:username, <span class="string">'upass'</span>:password, <span class="string">'0MKKey'</span>:<span class="string">'login'</span>&#125;, verify=<span class="keyword">False</span>)</div><div class="line">	print(<span class="string">"请求中...正在登陆......"</span>)</div><div class="line">	<span class="keyword">for</span> each <span class="keyword">in</span> tqdm(range(<span class="number">1</span>,<span class="number">100</span>)):</div><div class="line">		sleep(<span class="number">0.015</span>)</div><div class="line"><span class="keyword">except</span> :</div><div class="line">	<span class="keyword">print</span> (<span class="string">'出现错误...请联系管理员......'</span>)</div></pre></td></tr></table></figure></p>
<p><em>requests.post</em>将<em>data</em>post到URL，DDDDD处为用户名，upass为密码。<br>这里except防止出现网络问题。</p>
<p>之后就可以获得网页上的信息了，但源代码上似乎找不到流量的信息，看了半天，原来需要自己计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">flow0 = flow%<span class="number">1024</span></div><div class="line">flow1 = flow-flow0</div><div class="line">flow0 = flow0*<span class="number">1000</span></div><div class="line">flow0 = flow0-(flow0 % <span class="number">1024</span>)</div><div class="line">flow3=<span class="string">'.'</span></div><div class="line"><span class="keyword">if</span> (flow0/<span class="number">1024</span>)&lt;<span class="number">10</span>:</div><div class="line">	flow3=<span class="string">'.00'</span></div><div class="line"><span class="keyword">elif</span> (flow0/<span class="number">1024</span>)&lt;<span class="number">100</span>:</div><div class="line">	flow3=<span class="string">'.0'</span></div><div class="line">flow = str (int(flow1/<span class="number">1024</span>)) + str (flow3)+ str (int(flow0/<span class="number">1024</span>))</div></pre></td></tr></table></figure>
<p>这样就可以了，再获取名字和使用时长的字段，最后po出来就行了。</p>
<p>如果我想要断开连接呢？于是我又在主函数后加上了一段：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">	co = input(<span class="string">"(输入'N'断开当前连接：)"</span>)</div><div class="line">	<span class="keyword">if</span> co == <span class="string">"N"</span>:</div><div class="line">		url = <span class="string">"http://202.205.80.10/F.htm"</span></div><div class="line">		send_headers = &#123;</div><div class="line">				<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.80 Safari/537.36'</span>,</div><div class="line">				<span class="string">'Accept'</span>: <span class="string">'*/*'</span>,</div><div class="line">				<span class="string">'Connection'</span>: <span class="string">'keep-alive'</span>,</div><div class="line">				<span class="string">'Host'</span>: <span class="string">'202.205.80.10'</span></div><div class="line">		&#125;</div><div class="line">		ss = requests.get(url, headers=send_headers)</div><div class="line">		<span class="keyword">print</span> (<span class="string">"已断开当前连接..."</span>)</div><div class="line">		<span class="keyword">break</span></div></pre></td></tr></table></figure></p>
<p>这里是一个死循环，只有输入的字符为<em>N</em>才能跳出循环，即断开连接。</p>
<hr>
<h1 id="Packaging"><a href="#Packaging" class="headerlink" title="Packaging"></a>Packaging</h1><p>好东西不能独享，但别人电脑也不一定有装Python啊，于是上网找了个<em>PyInstall</em>用来打包成<em>exe</em>文件。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">f = open(<span class="string">"账号密码.txt"</span>, <span class="string">"r"</span>)</div><div class="line">	content = f.read()</div><div class="line">	fs = content.find(<span class="string">'username:'</span>) + len(<span class="string">'username:'</span>)</div><div class="line">	username = content[(content.find(<span class="string">'username:'</span>) + len(<span class="string">'username:'</span>)):(content.find(<span class="string">"\n"</span>,fs))]</div><div class="line">	password = content[(content.find(<span class="string">'password:'</span>) + len(<span class="string">'password:'</span>)):]</div></pre></td></tr></table></figure></p>
<p>又在主函数前加了储存账号密码的一段程序，将<em>账号密码.txt</em>放在与此程序一个目录下，格式为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">username:#你的账号</div><div class="line">password:#你的密码</div></pre></td></tr></table></figure></p>
<p>程序每次启动时自动读取就行了。</p>
<hr>
<h1 id="testing"><a href="#testing" class="headerlink" title="testing"></a>testing</h1><p>因为是命令行程序，毕竟难看了点：<br><img src="http://i4.piimg.com/567571/b5c6a587a2eb95d8.png" alt=""><br>但还是很好用哒！给实验室几个师兄用了一下，都说好:-)</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Documentation of scikit-learn 0.17]]></title>
      <url>http://yoursite.com/2016/09/14/Documentation-of-scikit-learn-0-17/</url>
      <content type="html"><![CDATA[<p>准备学习一下Machinelearning，看到Python中sk-learn这个库挺不错的，准备用它试试手。<br>顺便翻译一下<a href="http://scikit-learn.org/stable/documentation.html" target="_blank" rel="external">文档</a>。<br><a id="more"></a></p>
<hr>
<h1 id="学习及预测"><a href="#学习及预测" class="headerlink" title="学习及预测"></a>学习及预测</h1><p>在数字数据集的情况下，给定一个数字图片用于预测。样本分为十个等级（即数字零到数字九），我们通过<em>匹配</em>一个规则来<em>预测</em>未出现的样本属于哪个等级。<br>在sk-learn，归类规则为一个Python对象，使用<strong>fit(X,y)</strong>和<strong>predict(T)</strong>方法来实现。</p>
<p>一个规则的例子就是<strong>sklearn.svm.SVC</strong>即<a href="http://en.wikipedia.org/wiki/Support_vector_machine" target="_blank" rel="external">支持向量机</a>。规则的组成即模型的参数，但仅目前来说，我们把此规则视为一个黑箱。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf = svm.SVC(gamma=<span class="number">0.001</span>, C=<span class="number">100.</span>)</div></pre></td></tr></table></figure></p>
<blockquote>
<p><em>挑选模型参数</em><br>在本例中我们手动设置gamma值。也可以使用一些如grid search和cross validation的工具自动寻找合适的参数值。</p>
</blockquote>
<p>定义我们的规则为<strong>clf</strong>，它必须符合我们的模型，从模型中进行学习。训练集数据为除了最后一个外的所有数据。Python语法为<strong>[:-1]</strong>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf.fit(digits.data[:<span class="number">-1</span>], digits.target[:<span class="number">-1</span>])  </div><div class="line">SVC(C=<span class="number">100.0</span>, cache_size=<span class="number">200</span>, class_weight=<span class="keyword">None</span>, coef0=<span class="number">0.0</span>,</div><div class="line">  decision_function_shape=<span class="keyword">None</span>, degree=<span class="number">3</span>, gamma=<span class="number">0.001</span>, kernel=<span class="string">'rbf'</span>,</div><div class="line">  max_iter=<span class="number">-1</span>, probability=<span class="keyword">False</span>, random_state=<span class="keyword">None</span>, shrinking=<span class="keyword">True</span>,</div><div class="line">  tol=<span class="number">0.001</span>, verbose=<span class="keyword">False</span>)</div></pre></td></tr></table></figure></p>
<p>现在我们可以预测新值，测试询问我们的分类器测试集为哪个数字。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf.predict(digits.data[<span class="number">-1</span>:])</div><div class="line">array([<span class="number">8</span>])</div></pre></td></tr></table></figure></p>
<p>图片为：<br><img src="http://i4.piimg.com/567571/c885c6157fcf1247.png" alt=""></p>
<p>如你所见，这是一个很有挑战性的工作，此图十分棘手，你同意分类器的<a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html" target="_blank" rel="external">结果</a>吗？</p>
<h1 id="持久模型"><a href="#持久模型" class="headerlink" title="持久模型"></a>持久模型</h1><p>你可以使用Python内建持久化模块<a href="http://docs.python.org/library/pickle.html" target="_blank" rel="external">pickle</a>来对scikit中的模型进行保存：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf = svm.SVC()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>iris = datasets.load_iris()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>X, y = iris.data, iris.target</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf.fit(X, y)  </div><div class="line">SVC(C=<span class="number">1.0</span>, cache_size=<span class="number">200</span>, class_weight=<span class="keyword">None</span>, coef0=<span class="number">0.0</span>,</div><div class="line">  decision_function_shape=<span class="keyword">None</span>, degree=<span class="number">3</span>, gamma=<span class="string">'auto'</span>, kernel=<span class="string">'rbf'</span>,</div><div class="line">  max_iter=<span class="number">-1</span>, probability=<span class="keyword">False</span>, random_state=<span class="keyword">None</span>, shrinking=<span class="keyword">True</span>,</div><div class="line">  tol=<span class="number">0.001</span>, verbose=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> pickle</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>s = pickle.dumps(clf)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf2 = pickle.loads(s)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf2.predict(X[<span class="number">0</span>:<span class="number">1</span>])</div><div class="line">array([<span class="number">0</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y[<span class="number">0</span>]</div><div class="line"><span class="number">0</span></div></pre></td></tr></table></figure></p>
<p>在某些特殊情况下，使用joblib（<strong>joblib.dump</strong> &amp; <strong>joblib.load</strong>）来替代pickle对大数据集更有效率：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>joblib.dump(clf, <span class="string">'filename.pkl'</span>)</div></pre></td></tr></table></figure></p>
<p>之后你可以载入pickled模型（在另一Python进程中）通过：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf = joblib.load(<span class="string">'filename.pkl'</span>)</div></pre></td></tr></table></figure></p>
<blockquote>
<p>NOTE：joblib.dump返回一个文件名列表。在clf对象中每个独立的numpy数组在文件系统中被序列化为独立的文件。所有在同一目录下的文件在被joblib.load读取模型时都被用到。</p>
</blockquote>
<p>需注意的是，pickle有一些安全及可维护性的问题。请参照 <a href="http://scikit-learn.org/stable/modules/model_persistence.html#model-persistence" target="_blank" rel="external">持久模型</a> 一节了解sk-learn中关于此类的更多信息。</p>
<h1 id="常用惯例"><a href="#常用惯例" class="headerlink" title="常用惯例"></a>常用惯例</h1><p>sk-learn 估计量遵循一些确定的规则使其预测。</p>
<ul>
<li>类型转换<br>除其他特殊情况，输入类型为 <strong>float64</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn <span class="keyword">import</span> random_projection</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>rng = np.random.RandomState(<span class="number">0</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>X = rng.rand(<span class="number">10</span>, <span class="number">2000</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>X = np.array(X, dtype=<span class="string">'float32'</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>X.dtype</div><div class="line">dtype(<span class="string">'float32'</span>)</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>transformer = random_projection.GaussianRandomProjection()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>X_new = transformer.fit_transform(X)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>X_new.dtype</div><div class="line">dtype(<span class="string">'float64'</span>)</div></pre></td></tr></table></figure>
</li>
</ul>
<p>在此例中，X是<strong>float32</strong>，之后通过<strong>fit_transform(X)</strong>转化为<strong>float64</strong>。</p>
<p>回归分析目标被转化为<strong>float64</strong>，归类目标则使用如下方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>iris = datasets.load_iris()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf = SVC()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf.fit(iris.data, iris.target)  </div><div class="line">SVC(C=<span class="number">1.0</span>, cache_size=<span class="number">200</span>, class_weight=<span class="keyword">None</span>, coef0=<span class="number">0.0</span>,</div><div class="line">  decision_function_shape=<span class="keyword">None</span>, degree=<span class="number">3</span>, gamma=<span class="string">'auto'</span>, kernel=<span class="string">'rbf'</span>,</div><div class="line">  max_iter=<span class="number">-1</span>, probability=<span class="keyword">False</span>, random_state=<span class="keyword">None</span>, shrinking=<span class="keyword">True</span>,</div><div class="line">  tol=<span class="number">0.001</span>, verbose=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>list(clf.predict(iris.data[:<span class="number">3</span>]))</div><div class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf.fit(iris.data, iris.target_names[iris.target])  </div><div class="line">SVC(C=<span class="number">1.0</span>, cache_size=<span class="number">200</span>, class_weight=<span class="keyword">None</span>, coef0=<span class="number">0.0</span>,</div><div class="line">  decision_function_shape=<span class="keyword">None</span>, degree=<span class="number">3</span>, gamma=<span class="string">'auto'</span>, kernel=<span class="string">'rbf'</span>,</div><div class="line">  max_iter=<span class="number">-1</span>, probability=<span class="keyword">False</span>, random_state=<span class="keyword">None</span>, shrinking=<span class="keyword">True</span>,</div><div class="line">  tol=<span class="number">0.001</span>, verbose=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>list(clf.predict(iris.data[:<span class="number">3</span>]))  </div><div class="line">[<span class="string">'setosa'</span>, <span class="string">'setosa'</span>, <span class="string">'setosa'</span>]</div></pre></td></tr></table></figure></p>
<p>在这里，第一个<strong>predict()</strong>返回一个整型队列，因为<strong>iris.target</strong>(一个整型队列)被用在<em>fit</em>中。第二个<strong>predict</strong>返回一个字符串队列，因为iris.target_names被用于匹配。</p>
<ul>
<li>改装和升级参数</li>
</ul>
<p>模拟器的超参数可以在通过使用 <a href="http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline.set_params" target="_blank" rel="external">sklearn.pipeline.Pipeline.set_params</a> 方法建成后进行升级。多次使用fit()将覆盖之前fit()学习到的内容：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>rng = np.random.RandomState(<span class="number">0</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>X = rng.rand(<span class="number">100</span>, <span class="number">10</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y = rng.binomial(<span class="number">1</span>, <span class="number">0.5</span>, <span class="number">100</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>X_test = rng.rand(<span class="number">5</span>, <span class="number">10</span>)</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf = SVC()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf.set_params(kernel=<span class="string">'linear'</span>).fit(X, y)  </div><div class="line">SVC(C=<span class="number">1.0</span>, cache_size=<span class="number">200</span>, class_weight=<span class="keyword">None</span>, coef0=<span class="number">0.0</span>,</div><div class="line">  decision_function_shape=<span class="keyword">None</span>, degree=<span class="number">3</span>, gamma=<span class="string">'auto'</span>, kernel=<span class="string">'linear'</span>,</div><div class="line">  max_iter=<span class="number">-1</span>, probability=<span class="keyword">False</span>, random_state=<span class="keyword">None</span>, shrinking=<span class="keyword">True</span>,</div><div class="line">  tol=<span class="number">0.001</span>, verbose=<span class="keyword">False</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf.predict(X_test)</div><div class="line">array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>])</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf.set_params(kernel=<span class="string">'rbf'</span>).fit(X, y)  </div><div class="line">SVC(C=<span class="number">1.0</span>, cache_size=<span class="number">200</span>, class_weight=<span class="keyword">None</span>, coef0=<span class="number">0.0</span>,</div><div class="line">  decision_function_shape=<span class="keyword">None</span>, degree=<span class="number">3</span>, gamma=<span class="string">'auto'</span>, kernel=<span class="string">'rbf'</span>,</div><div class="line">  max_iter=<span class="number">-1</span>, probability=<span class="keyword">False</span>, random_state=<span class="keyword">None</span>, shrinking=<span class="keyword">True</span>,</div><div class="line">  tol=<span class="number">0.001</span>, verbose=<span class="keyword">False</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf.predict(X_test)</div><div class="line">array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>])</div></pre></td></tr></table></figure></p>
<p>在这里，默认kernal <strong>rbf</strong> 在模拟器通过<strong>SVC()</strong>建立之后一开始被改为 <strong>linear</strong>，之后被改回<strong>rbf</strong>以重新训练模拟器并做下次预测。</p>
<h1 id="未完待续，有空再更。"><a href="#未完待续，有空再更。" class="headerlink" title="未完待续，有空再更。"></a>未完待续，有空再更。</h1>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Voigtlander 50/2.8 dkl]]></title>
      <url>http://yoursite.com/2016/09/04/Voigtlander-50-2-8-dkl/</url>
      <content type="html"><![CDATA[<blockquote><p>Your first 10000 photographs are your worst.</p>
<footer><strong>Henri Cartier-Bresson</strong></footer></blockquote>
<a id="more"></a>
<p><strong>试片</strong></p>
<img src="/uploads/Voigtlander-50-2-8-dkl/DSC04053.jpg" class="full-image">
<img src="/uploads/Voigtlander-50-2-8-dkl/DSC06068.jpg" class="full-image">
<img src="/uploads/Voigtlander-50-2-8-dkl/DSC04231.jpg" class="full-image">]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Min.教你远离剧荒（2）]]></title>
      <url>http://yoursite.com/2016/09/03/Min-%E6%95%99%E4%BD%A0%E8%BF%9C%E7%A6%BB%E5%89%A7%E8%8D%92%EF%BC%882%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>上次我们已经获得了tag标签，现在我打算对标签进行排列组合。<br>这里仅计算组合结果，不进行排列，计算公式为：<br><img src="http://i1.piimg.com/567571/b82866d6d7abe928.png" alt=""></p>
<p>计算出所有tag在不同n情况下的组合，再用这些组合进行反向筛选电影，n越大权重越高，所以对n降序排列。<br>有些电影的tag数过多，所以决定只取5个tag。<br><a id="more"></a><br>先上代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="string">''' Mingo's movie_recommend script '''</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> itertools</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">arrange</span><span class="params">(tag)</span>:</span></div><div class="line">   <span class="keyword">for</span> num <span class="keyword">in</span> range(len(tag), <span class="number">0</span>, <span class="number">-1</span>):</div><div class="line">      s = (list(itertools.combinations(tag, num)))</div><div class="line">      print(s)</div></pre></td></tr></table></figure></p>
<p>这里我用到了itertools模块进行组合运算，itertools.combinations(tag, num)即从tag中取出num个元素进行组合，各组合return一个元组，list返回一个列表。这里range是反向排列的，因为num越大权重越大。输出如下：<br><img src="http://i1.piimg.com/567571/f0f362a7ca328610.png" alt=""></p>
<p>得到了标签的组合，现在再观察豆瓣电影标签查找电影的页面：<br><img src="http://i1.piimg.com/567571/c1e6043b0e01991c.png" alt=""></p>
<p>发现url的规律即：</p>
<pre><code>https://movie.douban.com/tag/
</code></pre><p>加上：<br>    东野圭吾%20推理%20日本%20福山雅治%20悬疑<br>其中的%20为空格，再对中文进行编码即可，于是修改代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">arrange</span><span class="params">(tag)</span>:</span></div><div class="line">   url_tag = <span class="string">"https://movie.douban.com/tag/"</span></div><div class="line">   <span class="keyword">for</span> num <span class="keyword">in</span> range(len(tag), <span class="number">0</span>, <span class="number">-1</span>):</div><div class="line">      s = (list(itertools.combinations(tag, num)))</div><div class="line">      <span class="comment">#print(s)</span></div><div class="line">      <span class="keyword">for</span> each <span class="keyword">in</span> s:</div><div class="line">         url = url_tag + urllib.parse.quote(<span class="string">' '</span>.join(each))</div><div class="line">         <span class="keyword">print</span> (url)</div></pre></td></tr></table></figure></p>
<p>输出了通过所有的tag组合查找的网址。<br><img src="http://i1.piimg.com/567571/659fd8254f470d3d.png" alt=""></p>
<p>查看输出url的网页源代码，很容易找到了电影名所在位置。<br><img src="http://i1.piimg.com/567571/f06b850f2a7a4f9d.png" alt=""><br>但也有的tag组合可能查找不到电影，则查找不到a class：<br><img src="http://i1.piimg.com/567571/e2a7c25647de8646.png" alt=""><br>查找不到的即跳过，选出查找到的前十部即可。</p>
<p>直接上代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="string">''' Mingo's movie_recommend script '''</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">arrange</span><span class="params">(tag,url_pre)</span>:</span></div><div class="line"></div><div class="line">   url_tag = <span class="string">"https://movie.douban.com/tag/"</span></div><div class="line">   all = []</div><div class="line"></div><div class="line">   <span class="keyword">for</span> num <span class="keyword">in</span> range(len(tag), <span class="number">0</span>, <span class="number">-1</span>):</div><div class="line">      s = (list(itertools.combinations(tag, num)))</div><div class="line">      <span class="comment">#print(s)</span></div><div class="line">      <span class="keyword">for</span> each <span class="keyword">in</span> s:</div><div class="line">         url = url_tag + urllib.parse.quote(<span class="string">' '</span>.join(each))</div><div class="line">         html = get_html(url)</div><div class="line">         end = <span class="number">0</span></div><div class="line">         <span class="comment">#print (url)</span></div><div class="line">         <span class="keyword">for</span> i <span class="keyword">in</span> range(len(re.findall(<span class="string">'a class="nbg" href="'</span>, html))):</div><div class="line">               start = html.find(<span class="string">'a class="nbg" href="'</span>, end) + len(<span class="string">'a class="nbg" href="'</span>)</div><div class="line">               end = html.find(<span class="string">'"  title='</span>, start)</div><div class="line">               name_start = end + len(<span class="string">'"  title="'</span>)</div><div class="line">               name_end = html.find(<span class="string">'"&gt;'</span>, name_start)</div><div class="line"></div><div class="line">               <span class="keyword">if</span> html[start:end] != url_pre <span class="keyword">and</span> <span class="keyword">not</span> re.findall(html[start:end], <span class="string">''</span>.join(all)):</div><div class="line">                  all.append(html[name_start:name_end] + <span class="string">"\t"</span> + html[start:end] + <span class="string">"\n"</span>)</div><div class="line">                  <span class="keyword">if</span> len(all) == <span class="number">10</span>:</div><div class="line">                     <span class="keyword">return</span> <span class="string">''</span>.join(all)</div><div class="line">               <span class="comment">#print (html[name_start:name_end] + "\n" + html[start:end])</span></div><div class="line">               <span class="comment">#print (start,end,name_start,name_end)</span></div></pre></td></tr></table></figure></p>
<p>这里写的有些繁琐，首先arrange函数传入2个参数，tag为之前的标签，url_pre 为输入电影的地址。创建了一个all的list用于储存数据，url获得了标签组合的网址，利用之前的get_html()函数进行解析。之后在网页中查找地址，这里并没有用到BeautifulSoup，只用了普通的find查找。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">re.findall(<span class="string">'a class="nbg" href="'</span>, html)</div></pre></td></tr></table></figure></p>
<p>利用正则表达式查找出html电影的个数，进行循环。<br>start, end, name_start, name_end为4个数值，代表电影名起止位置及电影url起止位置。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> html[start:end] != url_pre <span class="keyword">and</span> <span class="keyword">not</span> re.findall(html[start:end], <span class="string">''</span>.join(all)):</div><div class="line">    all.append(html[name_start:name_end] + <span class="string">"\t"</span> + html[start:end] + <span class="string">"\n"</span>)</div><div class="line">    <span class="keyword">if</span> len(all) == <span class="number">10</span>:</div><div class="line">        <span class="keyword">return</span> <span class="string">''</span>.join(all)</div></pre></td></tr></table></figure></p>
<p>这里进行条件判断，两个条件：（1）要求此刻查找出的电影url与输入的电影url不同，这里就用到了arrange函数中传入的url_pre；（2）要求此刻查找出的电影url与all列表中已添加的电影url不同，以免最终结果中出现相同的电影。满足这两个条件即可将此电影url及名称添加进list。<br>添加完后再进行条件判断，如果添加个数达到10个则停止循环，直接返回all。</p>
<p>大致如此，让我们测试一下：<br><img src="http://i1.piimg.com/567571/a1781162b83465c6.png" alt=""><br><img src="http://i1.piimg.com/567571/31aae76f881e76fd.png" alt=""><br><img src="http://i1.piimg.com/567571/ceb0c22fb4a1a0cb.png" alt=""><br><img src="http://i1.piimg.com/567571/30622e6f229f00e3.png" alt=""></p>
<p>噢。。似乎还可以的样子。对输入单部影片来说，似乎就到这里可以了。。</p>
<p>但做单部影片也太没挑战性了，于是我决定在修改一下，在输入多部影片时，依旧能够找出最相近的影片。<br>首先多部影片就会有多个tag标签，不同影片可能会有相同的标签或不同的标签，所以需要搜集标签进行排序。<br>所以：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">tag_all = &#123;&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">tag_sort</span><span class="params">(tag,tag_all)</span>:</span></div><div class="line">   <span class="keyword">for</span> each <span class="keyword">in</span> tag:</div><div class="line">      <span class="keyword">if</span> each <span class="keyword">not</span> <span class="keyword">in</span> tag_all.keys():</div><div class="line">         tag_all[each] = <span class="number">1</span></div><div class="line">      <span class="keyword">else</span>:</div><div class="line">         tag_all[each]+=<span class="number">1</span></div><div class="line">   <span class="keyword">return</span> tag_all</div></pre></td></tr></table></figure></p>
<p>tag_sort()函数传入2个参数，tag为之前得到的标签，tag_all即为我们新创建的dict字典，用来储存tag并进行计数；tag为一个list，for each 遍历tag，进行条件判断：如果这个标签不存在于tag_all的key中，则将此标签添加进去；如果，存在，则将tag_all中此标签的values 加一。遍历完后此函数return一个tag_all字典。</p>
<p>为了配合，我们将主函数也修改了一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">   name = input(<span class="string">"movie name:"</span>)</div><div class="line">   name_list = name.split(<span class="string">','</span>)</div><div class="line">   tag_all = &#123;&#125;</div><div class="line">   <span class="keyword">for</span> name <span class="keyword">in</span> name_list:</div><div class="line">      <span class="comment">#print (name)</span></div><div class="line">      url_pre = <span class="string">"https://movie.douban.com/subject_search?search_text="</span> + urllib.parse.quote(name)</div><div class="line">      html = get_html(url_pre)</div><div class="line">      movie_url = analyse(html)</div><div class="line">      html_movie = get_html(movie_url)</div><div class="line">      movie_tag = tag(html_movie)</div><div class="line">      tag_all = tag_sort(movie_tag,tag_all)</div><div class="line">   <span class="keyword">print</span> (tag_all)</div><div class="line">   <span class="comment">#print (arrange(movie_tag, movie_url))</span></div></pre></td></tr></table></figure></p>
<p>电影名之间以”,”分隔，输出：<br><img src="http://i1.piimg.com/567571/4925f1d7494f8079.png" alt=""></p>
<p>看到输出的字典中喜剧，美国，动画的values最大，所以我们对字典进行排列：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">tag_dict = sorted(tag_all.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>], reverse=<span class="keyword">True</span>)</div><div class="line">movie_tag = []</div><div class="line"><span class="keyword">for</span> ta <span class="keyword">in</span> tag_dict[:<span class="number">5</span>]:</div><div class="line">   movie_tag.append(ta[<span class="number">0</span>])</div><div class="line"><span class="keyword">print</span> (movie_tag)</div></pre></td></tr></table></figure></p>
<p>这里tag_all.items()得到(键，值)的列表，sorted方法，通过key这个参数，reverse=TRUE表示降序。</p>
<p>最后输出：<br><img src="http://i1.piimg.com/567571/2d65fa295d095724.png" alt=""></p>
<p>看来没问题，排列输出正确。<br>将输出的list再拼接回去，再输出：<br><img src="http://i1.piimg.com/567571/ed4e250b40c64b93.png" alt=""><br><img src="http://i1.piimg.com/567571/85631317d1e0919b.png" alt=""><br><img src="http://i1.piimg.com/567571/1859e3b67e82c571.png" alt=""><br>okay，就这样完成了！效果似乎还不错。<br>于是你可以输入各种你喜欢的剧，看看这个小脚本到底给不给力吧！</p>
<p>最后把源代码上传到了github上。<br><a href="https://github.com/sdws1983" target="_blank" rel="external">Mingo’s Github</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Min.教你远离剧荒（1）]]></title>
      <url>http://yoursite.com/2016/08/11/Min-%E6%95%99%E4%BD%A0%E8%BF%9C%E7%A6%BB%E5%89%A7%E8%8D%92%EF%BC%881%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>假期嘛，就应该在家里吹着空调吃着冰棍看着剧。但是，剧荒了该怎么办！<br>好吧，我感觉最近也是没剧可看，于是我想着能不能自己写个荐剧的小脚本。<br><a id="more"></a><br>打开豆瓣电影，随便找一部影片打开。<br><img src="http://i2.buimg.com/567571/521009a7a5af14d7.png" alt=""></p>
<p>每部影片的界面大致如此，看看有什么可以利用来进行筛选的元素。<br>我大致的思路如此：输入一些自己喜欢的影片，找到这些影片的相似之处，之后再筛选推荐出我可能喜欢的影片。<br>于是我觉得可以利用右下角的标签过滤出自己喜欢的影片之间的相同点。<br>右键查看网页源代码，ctrl+F，查找标签：<br><img src="http://i2.buimg.com/567571/02b348bd575d5406.png" alt=""><br>okay，很容易地找到了标签的位置。</p>
<p>让我们从头开始，首先根据名字查找电影。在豆瓣电影搜索电影名字，很容易发现豆瓣就是地址栏后面加了search。搜索后页面出现了很多电影的名单，找第一个的地址即可，也是右键查看源代码：<br><img src="http://i2.buimg.com/567571/3b86518f782ed182.png" alt=""><br>发现了电影的地址，这里我使用Python3 + BeautifulSoup4直接得到，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="string">''' Mingo's movie_recommend script '''</span></div><div class="line"></div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">import</span> urllib.request</div><div class="line"><span class="keyword">import</span> urllib.parse</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_html</span><span class="params">(url)</span>:</span></div><div class="line"></div><div class="line">   send_headers = &#123;</div><div class="line">      <span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.80 Safari/537.36'</span>,</div><div class="line">      <span class="string">'Accept'</span>:<span class="string">'*/*'</span>,</div><div class="line">      <span class="string">'Connection'</span>:<span class="string">'keep-alive'</span>,</div><div class="line">      <span class="string">'Host'</span>:<span class="string">'movie.douban.com'</span></div><div class="line">   &#125;</div><div class="line"></div><div class="line">   req = urllib.request.Request(url,headers = send_headers)</div><div class="line">   response = urllib.request.urlopen(req)</div><div class="line">   html = response.read().decode(<span class="string">'utf-8'</span>)</div><div class="line"></div><div class="line">   <span class="keyword">return</span> html</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">analyse</span><span class="params">(html)</span>:</span></div><div class="line">   soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</div><div class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> soup.find_all(<span class="string">'a'</span>):</div><div class="line">      <span class="keyword">try</span>:</div><div class="line">         content = i[<span class="string">'href'</span>]</div><div class="line">         <span class="keyword">if</span> <span class="string">'/subject/'</span> <span class="keyword">in</span> content:</div><div class="line">            <span class="keyword">print</span> (content)</div><div class="line">            <span class="keyword">break</span></div><div class="line">      <span class="keyword">except</span>:</div><div class="line">         <span class="keyword">pass</span></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">   name = input(<span class="string">"movie name:"</span>)</div><div class="line">   url = <span class="string">"https://movie.douban.com/subject_search?search_text="</span> + urllib.parse.quote(name)</div><div class="line">   html = get_html(url)</div><div class="line">   analyse(html)</div></pre></td></tr></table></figure>
<p>这里需要注意的是url中的中文处理，这里使用<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">urllib.parse.quote()</div></pre></td></tr></table></figure></p>
<p>进行编码，BeautifulSoup直接获得标签内容。<br>输出结果得到电影地址，即：</p>
<pre><code>https://movie.douban.com/subject/21817627/
</code></pre><p>再获得得到的地址网页内容，根据上面的步骤获取标签：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="string">''' Mingo's movie_recommend script '''</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">tag</span><span class="params">(html)</span>:</span></div><div class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</div><div class="line">    tag = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> soup.find_all(<span class="string">'a'</span>):</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            content = i[<span class="string">'href'</span>]</div><div class="line">            <span class="keyword">if</span> <span class="string">'/tag/'</span> <span class="keyword">in</span> content:</div><div class="line">                tag.append(i.string)</div><div class="line">        <span class="keyword">except</span>:</div><div class="line">            <span class="keyword">pass</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> tag[<span class="number">1</span>:]</div></pre></td></tr></table></figure></p>
<p>输入电影地址，返回一个所有标签的列表，这样我们得到了tag：<br><img src="http://i2.buimg.com/567571/e6e3604402cb30a6.png" alt=""></p>
<p>于是我们准备再对tag进行处理，搜集所有影片的tag，之后对所有tag进行排列组合，再使用tag组合后进行反向搜索得到推荐电影，大致就是这个思路。</p>
<p>今天太迟了打算放到明天再继续。。</p>
<p>未完待续。。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Rookie's Quant-trading(1)]]></title>
      <url>http://yoursite.com/2016/08/05/Rookie-s-Quant-trading-1/</url>
      <content type="html"><![CDATA[<p><img src="http://i1.piimg.com/567571/8e5864d2fdceb0af.jpg" alt=""><br>量化交易似乎是一个很高大上的名词，隔行如隔山，以我短浅的股票投机经历，我也只能边学习边探讨。<br>也是从Crossin这个公众号受到启发，又跟师兄聊了一些投机经历，这两天开始小试牛刀一下。<br><a id="more"></a><br>平台选取在国内的：<br>    <em>优矿</em><br>    <a href="http://uqer.io" target="_blank" rel="external">uqer.io</a></p>
<p>进入，注册，然后开始研究，新建策略，这时候就给出了一个比较简单的策略。<br><img src="http://i1.piimg.com/567571/809b577337751962.png" alt=""></p>
<p>参考Crossin公众号的一些基本操作，打算自己先简单的编写一个策略试试手。<br>正好之前某天和师兄一起去下地，我们互相交流并探讨了一下关于股票投机的经历和一些操作观念。。从实验室聊到了车上，又从车上说到了地里。。总的来说，虽然我和师兄的投机观念有较大的不同，但也是可以求同存异的。<br>当天在地里，师兄就给我谈了谈他的一种交易手法。<br>基本是这样的：超短线进出，日跌幅小于3%，半仓；日跌幅大于3%小于6%，全仓；日跌幅大于6%，割肉止损；日涨幅大于9%，止盈。大概就是这些。<br>于是我试着把这个策略写了一下：<br><img src="http://i1.piimg.com/567571/20a052daed11c355.png" alt=""><br>简单解释一下：<br>start&amp;end：起始和结束时间，这里我是选择了一个2年的时间段。<br>universe：股池，这里我选择了师兄最喜欢交易的东航。<br>freq：策略类型，这里我选择的是日间交易<br>refresh_rate：调仓频率，由于是超短交易，所以我设为了每日交易。<br>initialize：账户初始化，暂时没有用到。<br>handle_data(account)：每次调仓的交易策略，最为主要。<br>    account.get_attribute_history：获取股票历时数据，这里传入的第一个参数“closePrice”为收盘价，第二个参数为10，即获取最近10天内该股的收盘价格，return一个list。<br>    account.universe：从全局变量universe和当前持有的股票池中，剔除了当天停牌、退市和数据异常股票的股池。<br>    hist[s][-1]/hist[s][-2]-1：最近交易日的日涨跌幅<br>    order_pct(s,0.5)：交易股票使其价值为账户总资产的50%，这里即半仓。<br>    order_to(s,0)：交易一定股票使交易后该股数量为0，即清仓。</p>
<p>大体就是如此，按此策略，我们试着跑了一下：<br><img src="http://i1.piimg.com/567571/24dfe884b4a7fc3a.png" alt=""><br>似乎还不错的样子，阿尔法达到了32.9%，应该说是轻松超过了基准，最大回撤还是有些大，达到了45.6%，不太理想，说明风险控制不足。于是我准备在此基础上再简单加一些风控机制。<br>即5日内跌幅大于10%或15日内涨幅超过50%时，全部清仓。<br>于是变成了：<br><img src="http://i1.piimg.com/567571/21c8e37d95511ce5.png" alt=""><br>再试着跑了一下：<br><img src="http://i1.piimg.com/567571/90ec267cb6bbe338.png" alt=""><br>看起来有效果了，阿尔法达到了62.9%，回撤也下降到了31.1%。从两图对比也可以看出在后半段15年8月左右的下跌中较好的控制了风险。</p>
<p>但我们也不能仅仅满足于此，股池内股票单一，只有一只东航，东航在14、15年涨幅巨大，本身就是只牛股，可能不太具有说服力，于是我又调仓换股，换个大蓝（烂）筹（丑），工商银行试试看。<br><img src="http://i1.piimg.com/567571/2ef731c4b539ddd0.png" alt=""><br>显然跟之前相比不尽如人意，想了想会不会是由于工行盘子太大，波动小导致此策略施展不开：）于是再换一只同样稳的不行的601857中国神油看看：<br><img src="http://i1.piimg.com/567571/132653e67386aeed.png" alt=""><br>果然也是不分上下，烂的抠脚啊。<br>既然如此，换成牛市后期和股灾中十分抢眼的国企改革龙头中粮生化看看：<br><img src="http://i1.piimg.com/567571/b3a14606ae444786.png" alt=""><br>嗯。。很惊人，再细看中粮这段时间的独立走势：<br><img src="http://i1.piimg.com/567571/90b4538335bf88bb.png" alt=""><br>发现策略在头肩顶结构后还能创出新高，嗯。。看来我做的这个简单风险控制策略还不错。。<br>当然我这只是粗略的写了一个小策略，也有许多不完善的地方，仓位的分散及股池内股票筛选、权重，都还没有制定，只是以一个简单的交易策略为依据，就当作练练手了。</p>
<p>可以参考：</p>
<p>阅读原文：Crossin的编程教室</p>
<p>优矿的API文档：<a href="https://uqer.io/help/faqApi/" target="_blank" rel="external">https://uqer.io/help/faqApi/</a></p>
<p>&amp;<br><img src="http://i1.piimg.com/567571/624d72ed1a888cdd.png" alt=""><br>讲了一些python做数据处理，之前有用R语言处理一些数据，虽然支持生信分析的包很多，但感觉用起来还是不太顺手，所以再准备学习一个。能用在平时学习上，顺便与量化交易做对接。</p>
<p>关于Quant-trading，我也只能闲暇时间学习一个，作为一个Rookie，我也只能偶尔写写心得，做一些微小的事情。</p>
]]></content>
    </entry>
    
  
  
</search>
