<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[从Fisher's exact test, pvalue, FDR 谈起]]></title>
      <url>http://yoursite.com/2017/05/07/%E4%BB%8EFisher-s-exact-test-pvalue-FDR-%E8%B0%88%E8%B5%B7/</url>
      <content type="html"><![CDATA[<p>事实上，我有一个形似这样的热图：<br><a id="more"></a><br><img src="http://i2.muimg.com/1949/d84d6ede0b16a644.png" alt=""></p>
<p>即一组 n 行 m 列的数据，在热图上，每一个小格代表每个数据占这一行总数的比例。</p>
<p>之前做到这里把热图一画，看看哪几个格子颜色深就得了，也想做一些统计学的比较，但似乎无从入手。</p>
<p>这几天看文献发现可以做一个 <strong>Fisher’s Exact Test</strong> ，其实也就是常说的富集分析，平时做的 GO 富集也就是这个道理，只是我没有想到而已。</p>
<p>什么是 <strong>Fisher’s exact test</strong> 呢？</p>
<blockquote><p>Fisher’s exact test is a statistical significance test used in the analysis of contingency tables. Although in practice it is employed when sample sizes are small, it is valid for all sample sizes</p>
<footer><strong>— From Wikipedia</strong></footer></blockquote>
<p>即是用于分析列联表 <em>contingency tables</em> 统计显著性检验方法，它用于检验两个分类的关联 <em>association</em> 。虽然实际中常常使用于小数据情况，但同样适用于大样本的情况。</p>
<table>
<thead>
<tr>
<th></th>
<th style="text-align:center">Men</th>
<th style="text-align:right">Women</th>
<th style="text-align:right">Row Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>Studying</td>
<td style="text-align:center">a</td>
<td style="text-align:right">b</td>
<td style="text-align:right">a+b</td>
</tr>
<tr>
<td>Not-studying</td>
<td style="text-align:center">c</td>
<td style="text-align:right">d</td>
<td style="text-align:right">c+d</td>
</tr>
<tr>
<td>Col Total</td>
<td style="text-align:center">a+c</td>
<td style="text-align:right">b+d</td>
<td style="text-align:right">a+b+c+d=n</td>
</tr>
</tbody>
</table>
<p>对一个2乘2列联表进行检验，使用公式：</p>
<p><img src="http://ww1.sinaimg.cn/large/006HJ39wgy1ffcschefbog30e4019dfm.gif" alt=""></p>
<p>这个 p 值即是这个表出现的概率，而 <strong>pvalue</strong> 表示在原假设为真前提下，出现该样本或比该样本更极端的概率之和，即 <strong>pvalue</strong> 是多个表 p 值的和。</p>
<p>现在转到生物学分析来，比如在做 GO 富集分析时，某样品 GO 注释的总基因数为 N ，属于某个条目的基因数为 M ，差异表达分析后得到的基因数量为 n ，这 n 个基因里有 k 个基因属于这个条目，用这些数据判断此条目是否富集。<br>关于富集性分析，有几种方法可以执行：</p>
<ul>
<li>超几何分布（常用）</li>
<li>fisher’s exact test （也使用超几何分布）</li>
<li>卡方检验</li>
</ul>
<p>这里<strong>超几何分布</strong>和 <strong>fisher’s exact test</strong> 是一个意思，<strong>fisher’s exact test</strong> 是使用超几何分布来计算 P 值，所以它们的结果是相同的，而<strong>卡方检验</strong>通常只做近似值估计，并不精确，不推荐使用。</p>
<p>试着用 <strong>fisher’s exact test</strong> 进行富集分析，先建立 2×2 列联表：</p>
<table>
<thead>
<tr>
<th></th>
<th style="text-align:center">gene.in.interest</th>
<th style="text-align:right">gene.not.in.interest</th>
</tr>
</thead>
<tbody>
<tr>
<td>In.category</td>
<td style="text-align:center">k</td>
<td style="text-align:right">M-k</td>
</tr>
<tr>
<td>Not.in.category</td>
<td style="text-align:center">n-k</td>
<td style="text-align:right">N-M-n+k</td>
</tr>
</tbody>
</table>
<p>在 <strong>R</strong> 中进行 <strong>fisher’s exact test</strong> ：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&gt; d &lt;- data.frame(gene.not.interest=c(M-k, N-M-n+k), gene.in.interest=c(k, n-k))</div><div class="line">&gt; row.names(d) &lt;- c(<span class="string">"In_category"</span>, <span class="string">"not_in_category"</span>)</div><div class="line"></div><div class="line">&gt; fisher.test(d)</div><div class="line"><span class="comment"># Fisher's Exact Test for Count Data</span></div><div class="line"><span class="comment"># data:  d </span></div><div class="line"><span class="comment"># p-value = 7.879e-10</span></div><div class="line"><span class="comment"># alternative hypothesis: true odds ratio is not equal to 1 </span></div><div class="line"><span class="comment"># 95 percent confidence interval:</span></div><div class="line"><span class="comment"># 0.1013210 0.3089718 </span></div><div class="line"><span class="comment"># sample estimates:</span></div><div class="line"><span class="comment"># odds ratio</span></div><div class="line"><span class="comment"># 0.1767937</span></div></pre></td></tr></table></figure>
<p>得到 <strong>pvalue</strong> 。</p>
<p>我比较用 <strong>Python</strong> ，在 <strong>Python</strong> 中也有相应的函数，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> fisher <span class="keyword">import</span> pvalue</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>mat = [[<span class="number">12</span>, <span class="number">5</span>], [<span class="number">29</span>, <span class="number">2</span>]]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>p = pvalue(<span class="number">12</span>, <span class="number">5</span>, <span class="number">29</span>, <span class="number">2</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>p.left_tail, p.right_tail, p.two_tail</div><div class="line"><span class="comment"># doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS </span></div><div class="line"><span class="comment"># (0.04455473783507..., 0.994525206021..., 0.0802685520741...)</span></div></pre></td></tr></table></figure>
<p>有两侧还有单侧的结果，选择合适的即可。</p>
<p>现在我们得到了这一 GO 条目的 P 值，仅仅是第一步，假设你根据 P 值挑选出了显著富集的 R 个 GO 条目，其中只有 S 个是真正富集的，另外的 V 个事实上是没有富集的，也就是假阳性，因此我们希望错误比例 <strong>Q＝V/R</strong> 平均而言不能超过某个预先设定的值（比如 0.05 ），在统计学上，这也就等价于控制 <strong>FDR</strong> 不能超过 <strong>5％</strong> 。</p>
<p>这里又提到一个问题，所谓 <strong>Pvalue</strong>、<strong>Qvalue</strong>、<strong>FDR</strong>、<strong>adjusted Pvalue</strong>究竟有啥区别，<strong>Pvalue</strong>之前介绍过了，即衡量假阳性率的指标，<strong>FDR</strong>如上所说，是衡量错误发现率的指标，即：使用<strong>Qvalue</strong>的这个参数预估<strong>FDR</strong>。<strong>Qvalue</strong> 是利用公式从<strong>Pvalue</strong>校正计算后得到，所以<strong>Qvalue</strong>通常又被称为<strong>adjusted Pvalue</strong>。</p>
<p>所以从某种程度来说，<strong>Qvalue</strong>、<strong>FDR</strong>、<strong>adjusted Pvalue</strong>是等价的。</p>
<p>接下来计算 <strong>FDR</strong>，<strong>R</strong>中的实现：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt; p&lt;-c(<span class="number">0.0003</span>,<span class="number">0.0001</span>,<span class="number">0.02</span>) </div><div class="line">&gt; p </div><div class="line"><span class="comment"># [1] 3e-04 1e-04 2e-02 </span></div><div class="line">&gt; p.adjust(p,method=<span class="string">"fdr"</span>,length(p)) </div><div class="line"><span class="comment"># [1] 0.00045 0.00030 0.02000</span></div></pre></td></tr></table></figure>
<p>在 <strong>Python</strong> 中使用 <em>statsmodels.sandbox.stats.multicomp.multipletests</em> 实现，这里就不细说了。</p>
<p>最后回到文初的问题，我把基因分成了 n 类，n 类的基因分别又注释到了 m 个条目类别里，现在拥有每类基因注释到每个条目的基因数 a，即是每个小格的意义，对每个小格做fisher’s exact test，得到P值，最后对每列，即每类基因集的所有P值做FDR检验，设定FDR阈值，即可获得富集小格。</p>
<p>写的比较急，可能有一些错误，请指正。</p>
<p><strong>参考</strong>:</p>
<p><em>超几何分析和 GO 富集分析</em><br><a href="http://www.bakerwm.com/r/2015/01/23/hypergeometric-analysis—enrichment-analysis/" target="_blank" rel="external">http://www.bakerwm.com/r/2015/01/23/hypergeometric-analysis—enrichment-analysis/</a></p>
<p><em>Fishers Exact Test for Python (Cython)</em><br><a href="https://github.com/brentp/fishers_exact_test" target="_blank" rel="external">https://github.com/brentp/fishers_exact_test</a></p>
<p><em>Statsmodels is a Python module</em><br><a href="http://www.statsmodels.org/devel/generated/statsmodels.sandbox.stats.multicomp.multipletests.html#statsmodels-sandbox-stats-multicomp-multipletests" target="_blank" rel="external">http://www.statsmodels.org/devel/generated/statsmodels.sandbox.stats.multicomp.multipletests.html#statsmodels-sandbox-stats-multicomp-multipletests</a></p>
<p><em>多重检验中的 FDR 错误控制方法与 p-value 的校正及 Bonferroni 校正</em><br><a href="https://wenku.baidu.com/view/c0008226a58da0116d17492e.html" target="_blank" rel="external">https://wenku.baidu.com/view/c0008226a58da0116d17492e.html</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[年度巨作！The big data of The Plant Cell : PC年度大数据抓取及分析(二)]]></title>
      <url>http://yoursite.com/2017/01/11/%E5%B9%B4%E5%BA%A6%E5%B7%A8%E4%BD%9C%EF%BC%81The-big-data-of-The-Plant-Cell-PC%E5%B9%B4%E5%BA%A6%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%93%E5%8F%96%E5%8F%8A%E5%88%86%E6%9E%90-%E4%BA%8C/</url>
      <content type="html"><![CDATA[<p>接上回说到爬虫已经基本成型，经过一些测试及debug，修改了一些地方。</p>
<p>首先还是编码的问题，一些人名地名出现什么德语法语拉丁语，易发生编码错误。<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line">reload(sys)</div><div class="line">sys.setdefaultencoding(<span class="string">'utf-8'</span>)</div></pre></td></tr></table></figure>
<p>Python2容易出现这种问题，所以把上面这段敲上去，解决问题一劳永逸，虽然有人不太推崇这种不安全的方式，但我真的懒得再把文本一个个转换了。。</p>
<p>之后在抓取摘要也出现一些问题，当指向一些 <strong>IN BRIEF</strong> 的<em>.abstract</em>网站时会跳转到<em>.short</em>，再直接抓取也会抓到多段文字。<br>因为只想抓取研究文章，所以加上判断，段落数大于1则不进行抓取。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</div><div class="line">contents = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> soup.find_all(<span class="string">'p'</span>):</div><div class="line">   <span class="comment">#print (i.string)</span></div><div class="line">   <span class="keyword">try</span>:</div><div class="line">      <span class="keyword">if</span> <span class="string">u"p-"</span> <span class="keyword">in</span> str(i[<span class="string">'id'</span>]):</div><div class="line">         content = str(i)[(str(i).find(<span class="string">"&gt;"</span>) + <span class="number">1</span>):(str(i).find(<span class="string">"&lt;/p&gt;"</span>, str(i).find(<span class="string">"&gt;"</span>) + <span class="number">1</span>))]</div><div class="line">         <span class="comment">#print (content)</span></div><div class="line">         content = re.sub(<span class="string">r'&lt;.*?&gt;'</span>, <span class="string">''</span>, str(content))</div><div class="line">         content = re.sub(<span class="string">r'\n'</span>, <span class="string">' '</span>, str(content))</div><div class="line">         content = re.sub(<span class="string">r' +'</span>, <span class="string">' '</span>, str(content))</div><div class="line">         <span class="keyword">if</span> len(content) &gt; <span class="number">250</span>:</div><div class="line">            contents.append(content)</div><div class="line">   <span class="keyword">except</span>:</div><div class="line">      <span class="keyword">pass</span></div><div class="line">   <span class="keyword">if</span> len(contents) &gt; <span class="number">1</span>:</div><div class="line">      <span class="keyword">print</span> (<span class="string">"content error"</span>)</div><div class="line">      contents = []</div><div class="line">      address_list = []</div><div class="line">      author_list = []</div></pre></td></tr></table></figure></p>
<p>基本上就没有问题啦，把程序跑上，然后吃个饭去。</p>
<p><img src="http://p1.bqimg.com/567571/6e84aaa7036ba283.jpg" alt=""></p>
<p>跑完了^_^统计一下总共有169篇文章被抓取，并把作者，机构，摘要分别归入不同的表中。</p>
<p>弄了一个很蠢的像<em>.fa</em>的文本结构，<em>&gt;146</em>为页数，并把每篇文章的机构前加上了顺序。<br><img src="http://p1.bqimg.com/567571/21601acef2bf39e2.jpg" alt=""></p>
<p>数据到手了，先从简单的开始，看一看作者的数据。<br>这里的作者我只抓取了最后一位通讯作者。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(input_file, output_file)</span>:</span></div><div class="line">   all = []</div><div class="line">   count = <span class="number">1</span></div><div class="line">   <span class="keyword">with</span> open (input_file) <span class="keyword">as</span> f:</div><div class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> f:</div><div class="line">         <span class="keyword">if</span> i[<span class="number">0</span>] == <span class="string">"&gt;"</span>:</div><div class="line">            <span class="keyword">pass</span></div><div class="line">         <span class="keyword">else</span>:</div><div class="line">            all.append(i[:<span class="number">-1</span>])</div><div class="line">   </div><div class="line">   <span class="keyword">print</span> (len(all))</div><div class="line"></div><div class="line">   tag_all = &#123;&#125;</div><div class="line">   <span class="keyword">for</span> each <span class="keyword">in</span> all:</div><div class="line">      <span class="keyword">if</span> each <span class="keyword">not</span> <span class="keyword">in</span> tag_all.keys():</div><div class="line">         tag_all[each] = <span class="number">1</span></div><div class="line">      <span class="keyword">else</span>:</div><div class="line">         tag_all[each] += <span class="number">1</span></div><div class="line"></div><div class="line">   <span class="keyword">print</span> (tag_all)</div><div class="line">   fou = open(<span class="string">'tmp.txt'</span>, <span class="string">'w'</span>)</div><div class="line">   </div><div class="line">   <span class="keyword">for</span> (k,v) <span class="keyword">in</span> tag_all.items():</div><div class="line">      tab =  str(k) + <span class="string">'\t'</span> + str(v) + <span class="string">'\n'</span></div><div class="line">      fou.write(str(tab))</div><div class="line">   fou.close()</div><div class="line"></div><div class="line">   data = pd.DataFrame(pd.read_table(<span class="string">'tmp.txt'</span>, names = [<span class="string">'a'</span>,<span class="string">'b'</span>]))</div><div class="line">   data = data.sort([<span class="string">'b'</span>],axis = <span class="number">0</span>, ascending = <span class="keyword">False</span>)</div><div class="line">   data.to_csv(output_file, sep=<span class="string">'\t'</span>)</div><div class="line">   os.popen(<span class="string">'rm tmp.txt'</span>)</div></pre></td></tr></table></figure>
<p>首先数据清洗，把”&gt;”页码标记给去除，只留下作者。把作者及作者出现次数储存进字典里，最后使用<em>pandas</em>简单排序。</p>
<p>结果：<br><img src="http://p1.bqimg.com/567571/06f35e13a8317caa.jpg" alt=""></p>
<p>结果似乎差别都不大，最多的通讯作者一年也只是发了2篇文章，发表2篇文章的作者共有5位。<br>可以看到有国内清华的戚益军教授，其中还包含了一篇Review。<br><img src="http://p1.bqimg.com/567571/ed8ce013102320e5.jpg" alt=""></p>
<p>作者的数据似乎没有太多好挖掘的，就到这里了，所有作者数据及最后的结果统计我也放在<a href="https://github.com/sdws1983/The-big-data-OF-The-Plant-Cell" target="_blank" rel="external">github</a>上了，感兴趣的可以自己下载来玩。</p>
<p>接下来看看机构的数据。<br>将机构顺序进行分类：第n位的机构分别归入第n个文件。<br>先来看第一单位的数据，先不进行处理，直接进行统计：<br><img src="http://p1.bqimg.com/567571/ae46c86c0966852a.jpg" alt=""></p>
<p>发现有地名，机构，还有学院都混了进去，于是要进行一些数据清理。<br>观察原始数据，发现基本上倒数两个名称为城市或国家，之前的为机构，所以可以依此把机构和地区分开。</p>
<p>先来统计地区：<br><img src="http://p1.bqimg.com/567571/6487eb3d04ebdee4.jpg" alt=""></p>
<p>基本上还行，唯一的问题就是美国的文章是只注明州的，也许需要手工将这些数据合并，还有就是出现了多个China，包括China、P.R. China、PR China，我也懒得再改程序了，数据这么少我就人工在整理一下好了。</p>
<p>最后用ggplot2可视化：<br><img src="http://p1.bqimg.com/567571/4a57f2bf0c7b6571.jpg" alt=""></p>
<p>论文大户依然是几个老牌发达国家及中国，米国依然是稳居第一，总体来说也没有什么令人惊讶的地方。</p>
<p>再把目光放到机构的数据上，之前因为数据中含有大量的学院啊、系啊、实验室等元素，我们只需要大学或研究院的数据，所以将其进行过滤：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> <span class="string">u'Department'</span> <span class="keyword">in</span> each <span class="keyword">or</span> <span class="string">u'College'</span> <span class="keyword">in</span> each <span class="keyword">or</span> <span class="string">u'School'</span> <span class="keyword">in</span> each <span class="keyword">or</span> <span class="string">u'Institute'</span> <span class="keyword">in</span> each <span class="keyword">or</span> <span class="string">u'Laboratory'</span> <span class="keyword">in</span> each:</div><div class="line">   <span class="keyword">pass</span></div><div class="line"><span class="keyword">else</span>:</div><div class="line">   all.append(each)</div></pre></td></tr></table></figure></p>
<p>得到：<br><img src="http://p1.bqimg.com/567571/3b7d9da3b1a06863.jpg" alt=""></p>
<p>这里有一些值得注意的地方，把INRA和CNRA在原文件中进行查找：<br><img src="http://p1.bqimg.com/567571/ccbb125d65795bcc.jpg" alt=""></p>
<p>发现全部指向同一个学校 <strong>Université Paris-Saclay</strong>。<br>这个学校是什么一种存在呢？搜了一搜（来自百度）：<br><blockquote><p>巴黎-萨克雷大学（Université Paris-Saclay）是一所于2014年12月29日在法国巴黎南郊成立的的一所巨型大学 。<br>巴黎-萨克雷合并了2所大学、10所大学校（Grande Ecole）与7个研究所，其中包括巴黎综合理工学院（Ecole Polytechnique）、巴黎高等商业研究学院（HEC Paris）、巴黎中央理工-高等电力学院(Centrale-Supélec)和巴黎第十一大学（Université Paris-Sud）、凡尔赛大学（Université de Versailles-Saint-Quentin-en-Yvelines)等。校园面积达1350英亩，有约60000名学生与10500名科研人员</p>
<p>七个研究所：<br>CNRS（国立中央科学研究所）<br>CEA（原子能研究所）<br>IHES（高等研究所）<br>INRA（国立农业研究所）<br>INRIA（国立计算机与自动化研究所）<br>ONERA（国立航空研究办公室）<br>INSERM（国立健康与药物研究所）</p>
</blockquote></p>
<p>大意就是法国的学校实在是小而精，整合成这么一个共同体有利于提高世界排名。</p>
<p>于是把这些都合并到 <strong>Université Paris-Saclay</strong> 。<br>还有其他一些不好识别的名称，手动进行整合。</p>
<p>最后：<br><img src="http://p1.bqimg.com/567571/8c3e91e24a08267c.jpg" alt=""></p>
<p>进行一下简单的解读，第一位是中科院系统，随后是比利时VIB研究所，美国文章虽然多，但机构较为分散。<br>不对此排名发表任何看法。</p>
<p>代码、数据、图片都上传到<a href="https://github.com/sdws1983/The-big-data-OF-The-Plant-Cell" target="_blank" rel="external">github</a>上。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[年度巨作！The big data of The Plant Cell : PC年度大数据抓取及分析(一)]]></title>
      <url>http://yoursite.com/2017/01/09/%E5%B9%B4%E5%BA%A6%E5%B7%A8%E4%BD%9C%EF%BC%81The-big-data-of-The-Plant-Cell-PC%E5%B9%B4%E5%BA%A6%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%93%E5%8F%96%E5%8F%8A%E5%88%86%E6%9E%90-%E4%B8%80/</url>
      <content type="html"><![CDATA[<p>只是突然冒出这么一个想法，在读文献的时候。<br>好久没写爬虫了，顺便还可以练练R。</p>
<p><em>The Plant Cell</em>，1989年创刊，月刊，5年<em>IF</em>：10.529。<br>毫无疑问是植物学最顶级的杂志，植物人的梦想。<br><a id="more"></a></p>
<p>我大概的目标是抓取每一篇研究文章的摘要，顺带通讯作者和作者所在机构，之后看看用分词的包把摘要进行分词，最后计算词频，以及作者、机构的频数。</p>
<p>首先进入PC网站，随便找一篇文章进入它的摘要界面：<br><img src="http://p1.bqimg.com/567571/5f7f6cf8fcbabc52.jpg" alt=""></p>
<p>右键查看网页源代码，OK，可以爬取，顺带连作者机构都在一个页面，一起抓了。<br>再观察网页地址，发现<em>28/11/2715</em>，28为年份数，2016年的刊物都为28，11为月份数，代表11月，2715则为页码。</p>
<p>下图为2016年PC的月刊详情：<br><img src="http://p1.bqimg.com/567571/01ef43a484945aa1.jpg" alt=""></p>
<p>所以现在要从哪里获得这些文章的页码信息呢，发现在每一月期刊下有一个TOC：<br><img src="http://p1.bqimg.com/567571/bbbdb315b598ed56.jpg" alt=""></p>
<p>TOC就是table of content，点开一看是个PDF，就相当于目录的意思。<br><img src="http://p1.bqimg.com/567571/a9874f8f8cfa59d7.jpg" alt=""></p>
<p>目录上就有页码，似乎只要把这些页码爬下来就可以了。<br>但由于这个TOC是PDF，把它爬下来之后还不能直接解析，所以我又找了一个解析PDF文件的包，pdfminer，直接开搞！<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> urllib2 <span class="keyword">import</span> urlopen</div><div class="line"><span class="keyword">from</span> pdfminer.pdfinterp <span class="keyword">import</span> PDFResourceManager, PDFPageInterpreter</div><div class="line"><span class="keyword">from</span> pdfminer.converter <span class="keyword">import</span> TextConverter</div><div class="line"><span class="keyword">from</span> pdfminer.layout <span class="keyword">import</span> LAParams</div><div class="line"><span class="keyword">from</span> pdfminer.pdfpage <span class="keyword">import</span> PDFPage</div><div class="line"><span class="keyword">from</span> cStringIO <span class="keyword">import</span> StringIO</div><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_pdf_to_txt</span><span class="params">(fp)</span>:</span></div><div class="line">    rsrcmgr = PDFResourceManager()</div><div class="line">    retstr = StringIO()</div><div class="line">    codec = <span class="string">'utf-8'</span></div><div class="line">    laparams = LAParams()</div><div class="line">    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)</div><div class="line">    interpreter = PDFPageInterpreter(rsrcmgr, device)</div><div class="line">    password = <span class="string">""</span></div><div class="line">    maxpages = <span class="number">0</span></div><div class="line">    caching = <span class="keyword">True</span></div><div class="line">    pagenos=set()</div><div class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=<span class="keyword">True</span>):</div><div class="line">        interpreter.process_page(page)</div><div class="line"></div><div class="line">    fp.close()</div><div class="line">    device.close()</div><div class="line">    textstr = retstr.getvalue()</div><div class="line">    retstr.close()</div><div class="line">    <span class="keyword">return</span> textstr</div></pre></td></tr></table></figure></p>
<p>因为是在实验室服务器上写的，环境是Python2.7。从网上参考的资料，把函数写好，最后返回的是解析的pdf，截取一小段解析的结果：<br><img src="http://p1.bqimg.com/567571/06d58bfba073b5ce.jpg" alt=""></p>
<p>效果不错，发现页码都是单独成行，这样就非常利于提取了。</p>
<p>再把页码提取出来，最后把一年的期刊都爬取一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_issues</span><span class="params">(iss)</span>:</span></div><div class="line">   </div><div class="line">   url=<span class="string">'http://www.plantcell.org/content/28/'</span> + str(iss) + <span class="string">'.toc.pdf'</span></div><div class="line">   fp = StringIO(urlopen(url).read())</div><div class="line">   text=convert_pdf_to_txt(fp)</div><div class="line">   text = str(text).split(<span class="string">"\n"</span>)</div><div class="line">   all = []</div><div class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> text:</div><div class="line">      <span class="comment">#print (i)</span></div><div class="line">      <span class="keyword">if</span> re.findall(<span class="string">'\D+'</span>, i):</div><div class="line">         <span class="keyword">pass</span></div><div class="line">      <span class="keyword">elif</span> i != <span class="string">''</span> :</div><div class="line">         all.append(i)</div><div class="line">   <span class="keyword">print</span> (all)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">   <span class="keyword">for</span> iss <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">12</span>):</div><div class="line">      <span class="comment">#print (iss)</span></div><div class="line">      get_issues(iss)</div></pre></td></tr></table></figure></p>
<p>split按回车分割text，再把text遍历，找出没有非数字出现的文本，并去除空字符，最后把每个页码添加进all列表，输出即可。</p>
<p>结果：<br><img src="http://p1.bqimg.com/567571/b548236b2940a0fc.jpg" alt=""></p>
<p>成功完成，这个获得页码的脚本就写好了，留在一边备用。</p>
<p>现在来解析文章的摘要等信息，回到摘要网页，查看源代码。<br>很简单的使用<em>BeautifulSoup4</em>把信息提取出来，过程就不赘述。<br><img src="http://p1.bqimg.com/567571/ef51c7b49658debb.jpg" alt=""></p>
<p>代码太多，不贴了，放在<a href="https://github.com/sdws1983/The-big-data-OF-The-Plant-Cell" target="_blank" rel="external">github</a>上。<br>花了半天的时间，今天就先做到这了，之后有空再继续。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[用微博实现远程关机]]></title>
      <url>http://yoursite.com/2016/10/01/%E7%94%A8%E5%BE%AE%E5%8D%9A%E5%AE%9E%E7%8E%B0%E8%BF%9C%E7%A8%8B%E5%85%B3%E6%9C%BA/</url>
      <content type="html"><![CDATA[<p>平时工作日电脑都是放在实验室，很少带回去，有的时候还要运行一些东西，人先回宿舍了，电脑也跟着熬了一宿夜。像我这种老破电脑，最怕逞强，对其损伤还是很大的。<br>前些天看了一些不错的思路，基本差不多，但他使用了微博的API，我觉得有点麻烦，于是换了自己的方法。<br><a id="more"></a></p>
<hr>
<h1 id="Thought"><a href="#Thought" class="headerlink" title="Thought"></a>Thought</h1><p>首先通过读取最新一条微博内容获取指令：设置一个关键词，当检测到微博出现关键词，即可触发关机指令；同时考虑只有新微博的指令才有效，进行一些逻辑判断。<br>关机是否顺利呢？这是在用到发送邮件的模块，成功关机则发送邮件到指定邮箱。<br>大致如此。</p>
<hr>
<h1 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h1><p>先来写获取微博内容的部分：<br>因为移动版微博的内容较少，速度也快点，所以抓的移动版微博，找到自己主页个人微博的地址。<br><em>headers</em>伪装，之后再使用<em>BeautifulSoup</em>及<em>find</em>进行内容提取，比较简单。<br><em>json.loads</em>转化为字典，之后提取更加方便。<br>最后返回最新一条微博的发布时间<em>cr_time</em>，微博id号<em>id_</em>及微博内容<em>text</em>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weibo_content</span><span class="params">()</span>:</span></div><div class="line">	url = <span class="string">"http://m.weibo.cn/page/tpl?containerid=XXXXXXXXXXXXXXX_-_WEIBO_SECOND_PROFILE_WEIBO"</span></div><div class="line">	send_headers = &#123;</div><div class="line">		<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.80 Safari/537.36'</span>,</div><div class="line">		<span class="string">'Accept'</span>: <span class="string">'*/*'</span>,</div><div class="line">		<span class="string">'Connection'</span>: <span class="string">'keep-alive'</span>,</div><div class="line">		<span class="string">'Host'</span>: <span class="string">'m.weibo.cn'</span>,</div><div class="line">    	&#125;</div><div class="line"></div><div class="line">	r = requests.get(url, headers=send_headers)</div><div class="line">	data = r.text</div><div class="line">	<span class="comment">#print (data)</span></div><div class="line">	soup = BeautifulSoup(data,<span class="string">'lxml'</span>)</div><div class="line">	content = soup.find_all(<span class="string">'script'</span>)[<span class="number">1</span>].string</div><div class="line">	<span class="comment">#print (content)</span></div><div class="line">	st = content.find(<span class="string">'&#123;"card_type"'</span>)</div><div class="line">	en = content.find(<span class="string">'&#123;"card_type"'</span>,st+<span class="number">2</span>) <span class="number">-1</span></div><div class="line">	content = content[st:en]</div><div class="line">	<span class="comment">#print (content)</span></div><div class="line"></div><div class="line">	content = json.loads(content)</div><div class="line">	cr_time = content[<span class="string">'mblog'</span>][<span class="string">'created_at'</span>]</div><div class="line">	id_ = content[<span class="string">'mblog'</span>][<span class="string">'id'</span>]</div><div class="line">	text = content[<span class="string">'mblog'</span>][<span class="string">'text'</span>]</div><div class="line">	text = urllib.parse.unquote(text)</div><div class="line">	<span class="comment">#print (text)</span></div><div class="line">	<span class="keyword">return</span> cr_time, id_, text</div></pre></td></tr></table></figure></p>
<p>之后再写一下发送邮件的部分：<br><em>SMTP_host</em>为发送邮件地址的服务器，<em>from_addr</em>为发送邮件地址，<em>password</em>为发送邮件邮箱密码，<em>to_addrs</em>为接收邮件地址，<em>subject</em>为邮件主题，<em>content</em>为邮件内容。<br>比较简单。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> smtplib <span class="keyword">import</span> SMTP</div><div class="line"><span class="keyword">from</span> email.mime.text <span class="keyword">import</span> MIMEText</div><div class="line"><span class="keyword">from</span> email.header <span class="keyword">import</span> Header</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">send_email</span><span class="params">(SMTP_host, from_addr, password, to_addrs, subject, content)</span>:</span></div><div class="line">	email_client = SMTP(SMTP_host)</div><div class="line">	email_client.login(from_addr, password)</div><div class="line">	<span class="comment"># create msg</span></div><div class="line">	msg = MIMEText(content, <span class="string">'plain'</span>, <span class="string">'utf-8'</span>)</div><div class="line">	msg[<span class="string">'Subject'</span>] = Header(subject, <span class="string">'utf-8'</span>)<span class="comment">#subject</span></div><div class="line">	msg[<span class="string">'From'</span>] = from_addr</div><div class="line">	msg[<span class="string">'To'</span>] = to_addrs</div><div class="line">	email_client.sendmail(from_addr, to_addrs, msg.as_string())</div><div class="line"></div><div class="line">	email_client.quit()</div></pre></td></tr></table></figure>
<p>最后连接一下：<br>先获取之前的最新微博信息，进入循环，每次刷新再获得最新的微博信息，并进行比较，当最新微博id改变且新微博内容中有 <em>“shutdown”</em>这个关键词时，进行关机。<br>关机命令<em>os.system(‘shutdown -s -t 10’)</em>为10秒后关机，具体命令可再查询。<br><em>time.strftime(‘%H:%M:%S’)</em>为当前时间。<br>最后<em>sleep(20)</em>，表示每20秒执行一次。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">	cr_time, id_, text = get_weibo_content()</div><div class="line"></div><div class="line">	<span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">		cr_time_n, id_n, text_n = get_weibo_content()</div><div class="line">		<span class="keyword">print</span> (id_n)</div><div class="line">		<span class="keyword">print</span> (text_n)</div><div class="line"></div><div class="line">		<span class="keyword">if</span> str(id_n) != str(id_) <span class="keyword">and</span> (<span class="string">u'shutdown'</span> <span class="keyword">in</span> text_n):</div><div class="line">			mytime = <span class="string">'shutdown time : '</span> + str(time.strftime(<span class="string">'%H:%M:%S'</span>)) + <span class="string">". Process finished.."</span></div><div class="line">			send_email(<span class="string">"smtp.XXX.com"</span>, <span class="string">"YYYYYY@XXX.com"</span>, <span class="string">"YOUR PASSWORD"</span>, <span class="string">"ZZZZZZ@AAA.com"</span>, <span class="string">"您的电脑关机成功！"</span>, mytime)</div><div class="line">			os.system(<span class="string">'shutdown -s -t 10'</span>)</div><div class="line"></div><div class="line">			<span class="keyword">break</span></div><div class="line">		</div><div class="line">		sleep(<span class="number">20</span>)</div></pre></td></tr></table></figure>
<p>大体如此：-）</p>
<hr>
<h1 id="Showing"><a href="#Showing" class="headerlink" title="Showing"></a>Showing</h1><p>首先运行程序，发布微博，带有关键词。<br><img src="http://p1.bpimg.com/567571/df4f455451b151cf.png" alt=""></p>
<p>等一等，然后：<br><img src="http://p1.bpimg.com/567571/b6125fb7c8c9423a.png" alt=""><br>就是这么轻松：——）</p>
<hr>
<h1 id="Plus"><a href="#Plus" class="headerlink" title="Plus+"></a>Plus+</h1><p>这个自动发送邮件模块其实还是很好用的，如果你的邮箱有手机提醒的话就更完美了。<br>在此之前，我还应用这个邮件模块，写了一个自动查询火车票的脚本。</p>
<p><img src="http://i1.piimg.com/567571/2d373f6eb712b4a4.png" alt=""></p>
<p>通过它，我顺利的刷到了从某地回北京的硬卧票。。</p>
<p>怎么应用就看你了，希望你有一个不错的思路：——）</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[奥森阴天小游]]></title>
      <url>http://yoursite.com/2016/09/17/%E5%A5%A5%E6%A3%AE%E9%98%B4%E5%A4%A9%E5%B0%8F%E6%B8%B8/</url>
      <content type="html"><![CDATA[<p>中秋佳节，最后一日，与实验室友人游于奥森公园，天阴沉，并带相机小捏几张。<br><a id="more"></a></p>
<img src="/uploads/阴天头/Y3VyOUljK0lkdmhtcXdUMWVBV01iN01GUVByT1lDOFpkcmFzdVJGWWFPNitIZHg0YVkyUmZnPT0.jpg" class="full-image">
<img src="/uploads/阴天头/Y3VyOUljK0lkdmhtcXdUMWVBV01iOGEwSC9MMi9YWk5NbEMyMjgrRmNFdVRBcWZaQlArWHd3PT0.jpg" class="full-image">
<img src="/uploads/阴天头/Y3VyOUljK0lkdmhtcXdUMWVBV01iM1NGajFqaHVNb0hPYjR3M3FYaC9ScVFCbHQ5UXYzK0VnPT0.jpg" class="full-image">]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Min教你CAU一键上网]]></title>
      <url>http://yoursite.com/2016/09/15/CAU%E4%B8%80%E9%94%AE%E4%B8%8A%E7%BD%91/</url>
      <content type="html"><![CDATA[<p>到了新学校，搞不了什么大新闻，那至少也得完成点小目标。<br>一直觉得校园网这种登录方式浪费时间，打开浏览器，输入密码再跳转再输入，麻烦的不行。像哥这种日理万机的人，哪有时间干这种小事。于是前几周我花了一点下午的时间终于把这个自动登录的脚本写好并打包。<br>把整个思路记录梳理一下。<br><a id="more"></a></p>
<hr>
<h1 id="Simulation"><a href="#Simulation" class="headerlink" title="Simulation"></a>Simulation</h1><p>一般写爬虫的思路就是模拟浏览器的行为，cau的登录方式大致也还比较简单。</p>
<p>一打开浏览器，自动跳转至<a href="http://my.cau.edu.cn/" target="_blank" rel="external">my.cau.edu.cn</a>:<br><img src="http://i4.piimg.com/567571/27d7a95d2c735738.png" alt=""></p>
<p>之后点击左下角<strong>公共服务</strong>中的<strong>网络服务</strong>，进入了中国农业大学网络综合服务平台。<br><img src="http://i4.piimg.com/567571/491293e095d70eaa.png" alt=""></p>
<p>左边的网关登录，输入自己的账号密码即可。<br>登陆成功，显示网页：<br><img src="http://i4.piimg.com/567571/677731c53f9256f3.png" alt=""></p>
<p>整个过程基本如此。</p>
<hr>
<h1 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h1><p>发现登录这一步其实是向<em><a href="http://202.205.80.10/" target="_blank" rel="external">http://202.205.80.10/</a></em>post你的账号及密码：<br><img src="http://i4.piimg.com/567571/1258f66d21a4fc3c.png" alt=""><br>之后再get<em><a href="http://202.205.80.10/" target="_blank" rel="external">http://202.205.80.10/</a></em>的内容。<br>登录成功后网页有几个重要的信息，我们要获取它，比如名字，使用时间，使用流量等。</p>
<p>查看网页源代码，找到这几个信息的位置，获取它们即可。</p>
<hr>
<h1 id="Coding"><a href="#Coding" class="headerlink" title="Coding"></a>Coding</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</div><div class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</div></pre></td></tr></table></figure>
<p>导入需要的模块，这里用了<em>Requests</em>和<em>BeautifulSoup</em>，用来操控浏览器及网页信息获取。<br>为了使界面好看有意思一些，我还使用了一个进度条模块<em>tqdm</em>（虽然并没有必要）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">try</span>:</div><div class="line">	url = <span class="string">"http://202.205.80.10/"</span></div><div class="line">	send_headers = &#123;</div><div class="line">				<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.80 Safari/537.36'</span>,</div><div class="line">				<span class="string">'Accept'</span>:<span class="string">'*/*'</span>,</div><div class="line">				<span class="string">'Connection'</span>:<span class="string">'keep-alive'</span>,</div><div class="line">				<span class="string">'Host'</span>:<span class="string">'202.205.80.10'</span></div><div class="line">			&#125;</div><div class="line"></div><div class="line">	r = requests.post(url, headers = send_headers, data= &#123;<span class="string">'DDDDD'</span>:username, <span class="string">'upass'</span>:password, <span class="string">'0MKKey'</span>:<span class="string">'login'</span>&#125;, verify=<span class="keyword">False</span>)</div><div class="line">	print(<span class="string">"请求中...正在登陆......"</span>)</div><div class="line">	<span class="keyword">for</span> each <span class="keyword">in</span> tqdm(range(<span class="number">1</span>,<span class="number">100</span>)):</div><div class="line">		sleep(<span class="number">0.015</span>)</div><div class="line"><span class="keyword">except</span> :</div><div class="line">	<span class="keyword">print</span> (<span class="string">'出现错误...请联系管理员......'</span>)</div></pre></td></tr></table></figure></p>
<p><em>requests.post</em>将<em>data</em>post到URL，DDDDD处为用户名，upass为密码。<br>这里except防止出现网络问题。</p>
<p>之后就可以获得网页上的信息了，但源代码上似乎找不到流量的信息，看了半天，原来需要自己计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">flow0 = flow%<span class="number">1024</span></div><div class="line">flow1 = flow-flow0</div><div class="line">flow0 = flow0*<span class="number">1000</span></div><div class="line">flow0 = flow0-(flow0 % <span class="number">1024</span>)</div><div class="line">flow3=<span class="string">'.'</span></div><div class="line"><span class="keyword">if</span> (flow0/<span class="number">1024</span>)&lt;<span class="number">10</span>:</div><div class="line">	flow3=<span class="string">'.00'</span></div><div class="line"><span class="keyword">elif</span> (flow0/<span class="number">1024</span>)&lt;<span class="number">100</span>:</div><div class="line">	flow3=<span class="string">'.0'</span></div><div class="line">flow = str (int(flow1/<span class="number">1024</span>)) + str (flow3)+ str (int(flow0/<span class="number">1024</span>))</div></pre></td></tr></table></figure>
<p>这样就可以了，再获取名字和使用时长的字段，最后po出来就行了。</p>
<p>如果我想要断开连接呢？于是我又在主函数后加上了一段：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">	co = input(<span class="string">"(输入'N'断开当前连接：)"</span>)</div><div class="line">	<span class="keyword">if</span> co == <span class="string">"N"</span>:</div><div class="line">		url = <span class="string">"http://202.205.80.10/F.htm"</span></div><div class="line">		send_headers = &#123;</div><div class="line">				<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.80 Safari/537.36'</span>,</div><div class="line">				<span class="string">'Accept'</span>: <span class="string">'*/*'</span>,</div><div class="line">				<span class="string">'Connection'</span>: <span class="string">'keep-alive'</span>,</div><div class="line">				<span class="string">'Host'</span>: <span class="string">'202.205.80.10'</span></div><div class="line">		&#125;</div><div class="line">		ss = requests.get(url, headers=send_headers)</div><div class="line">		<span class="keyword">print</span> (<span class="string">"已断开当前连接..."</span>)</div><div class="line">		<span class="keyword">break</span></div></pre></td></tr></table></figure></p>
<p>这里是一个死循环，只有输入的字符为<em>N</em>才能跳出循环，即断开连接。</p>
<hr>
<h1 id="Packaging"><a href="#Packaging" class="headerlink" title="Packaging"></a>Packaging</h1><p>好东西不能独享，但别人电脑也不一定有装Python啊，于是上网找了个<em>PyInstall</em>用来打包成<em>exe</em>文件。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">f = open(<span class="string">"账号密码.txt"</span>, <span class="string">"r"</span>)</div><div class="line">	content = f.read()</div><div class="line">	fs = content.find(<span class="string">'username:'</span>) + len(<span class="string">'username:'</span>)</div><div class="line">	username = content[(content.find(<span class="string">'username:'</span>) + len(<span class="string">'username:'</span>)):(content.find(<span class="string">"\n"</span>,fs))]</div><div class="line">	password = content[(content.find(<span class="string">'password:'</span>) + len(<span class="string">'password:'</span>)):]</div></pre></td></tr></table></figure></p>
<p>又在主函数前加了储存账号密码的一段程序，将<em>账号密码.txt</em>放在与此程序一个目录下，格式为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">username:#你的账号</div><div class="line">password:#你的密码</div></pre></td></tr></table></figure></p>
<p>程序每次启动时自动读取就行了。</p>
<hr>
<h1 id="testing"><a href="#testing" class="headerlink" title="testing"></a>testing</h1><p>因为是命令行程序，毕竟难看了点：<br><img src="http://i4.piimg.com/567571/b5c6a587a2eb95d8.png" alt=""><br>但还是很好用哒！给实验室几个师兄用了一下，都说好:-)</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Documentation of scikit-learn 0.17]]></title>
      <url>http://yoursite.com/2016/09/14/Documentation-of-scikit-learn-0-17/</url>
      <content type="html"><![CDATA[<p>准备学习一下Machinelearning，看到Python中sk-learn这个库挺不错的，准备用它试试手。<br>顺便翻译一下<a href="http://scikit-learn.org/stable/documentation.html" target="_blank" rel="external">文档</a>。<br><a id="more"></a></p>
<hr>
<h1 id="学习及预测"><a href="#学习及预测" class="headerlink" title="学习及预测"></a>学习及预测</h1><p>在数字数据集的情况下，给定一个数字图片用于预测。样本分为十个等级（即数字零到数字九），我们通过<em>匹配</em>一个规则来<em>预测</em>未出现的样本属于哪个等级。<br>在sk-learn，归类规则为一个Python对象，使用<strong>fit(X,y)</strong>和<strong>predict(T)</strong>方法来实现。</p>
<p>一个规则的例子就是<strong>sklearn.svm.SVC</strong>即<a href="http://en.wikipedia.org/wiki/Support_vector_machine" target="_blank" rel="external">支持向量机</a>。规则的组成即模型的参数，但仅目前来说，我们把此规则视为一个黑箱。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf = svm.SVC(gamma=<span class="number">0.001</span>, C=<span class="number">100.</span>)</div></pre></td></tr></table></figure></p>
<blockquote>
<p><em>挑选模型参数</em><br>在本例中我们手动设置gamma值。也可以使用一些如grid search和cross validation的工具自动寻找合适的参数值。</p>
</blockquote>
<p>定义我们的规则为<strong>clf</strong>，它必须符合我们的模型，从模型中进行学习。训练集数据为除了最后一个外的所有数据。Python语法为<strong>[:-1]</strong>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf.fit(digits.data[:<span class="number">-1</span>], digits.target[:<span class="number">-1</span>])  </div><div class="line">SVC(C=<span class="number">100.0</span>, cache_size=<span class="number">200</span>, class_weight=<span class="keyword">None</span>, coef0=<span class="number">0.0</span>,</div><div class="line">  decision_function_shape=<span class="keyword">None</span>, degree=<span class="number">3</span>, gamma=<span class="number">0.001</span>, kernel=<span class="string">'rbf'</span>,</div><div class="line">  max_iter=<span class="number">-1</span>, probability=<span class="keyword">False</span>, random_state=<span class="keyword">None</span>, shrinking=<span class="keyword">True</span>,</div><div class="line">  tol=<span class="number">0.001</span>, verbose=<span class="keyword">False</span>)</div></pre></td></tr></table></figure></p>
<p>现在我们可以预测新值，测试询问我们的分类器测试集为哪个数字。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf.predict(digits.data[<span class="number">-1</span>:])</div><div class="line">array([<span class="number">8</span>])</div></pre></td></tr></table></figure></p>
<p>图片为：<br><img src="http://i4.piimg.com/567571/c885c6157fcf1247.png" alt=""></p>
<p>如你所见，这是一个很有挑战性的工作，此图十分棘手，你同意分类器的<a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html" target="_blank" rel="external">结果</a>吗？</p>
<h1 id="持久模型"><a href="#持久模型" class="headerlink" title="持久模型"></a>持久模型</h1><p>你可以使用Python内建持久化模块<a href="http://docs.python.org/library/pickle.html" target="_blank" rel="external">pickle</a>来对scikit中的模型进行保存：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf = svm.SVC()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>iris = datasets.load_iris()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>X, y = iris.data, iris.target</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf.fit(X, y)  </div><div class="line">SVC(C=<span class="number">1.0</span>, cache_size=<span class="number">200</span>, class_weight=<span class="keyword">None</span>, coef0=<span class="number">0.0</span>,</div><div class="line">  decision_function_shape=<span class="keyword">None</span>, degree=<span class="number">3</span>, gamma=<span class="string">'auto'</span>, kernel=<span class="string">'rbf'</span>,</div><div class="line">  max_iter=<span class="number">-1</span>, probability=<span class="keyword">False</span>, random_state=<span class="keyword">None</span>, shrinking=<span class="keyword">True</span>,</div><div class="line">  tol=<span class="number">0.001</span>, verbose=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> pickle</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>s = pickle.dumps(clf)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf2 = pickle.loads(s)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf2.predict(X[<span class="number">0</span>:<span class="number">1</span>])</div><div class="line">array([<span class="number">0</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y[<span class="number">0</span>]</div><div class="line"><span class="number">0</span></div></pre></td></tr></table></figure></p>
<p>在某些特殊情况下，使用joblib（<strong>joblib.dump</strong> &amp; <strong>joblib.load</strong>）来替代pickle对大数据集更有效率：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>joblib.dump(clf, <span class="string">'filename.pkl'</span>)</div></pre></td></tr></table></figure></p>
<p>之后你可以载入pickled模型（在另一Python进程中）通过：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf = joblib.load(<span class="string">'filename.pkl'</span>)</div></pre></td></tr></table></figure></p>
<blockquote>
<p>NOTE：joblib.dump返回一个文件名列表。在clf对象中每个独立的numpy数组在文件系统中被序列化为独立的文件。所有在同一目录下的文件在被joblib.load读取模型时都被用到。</p>
</blockquote>
<p>需注意的是，pickle有一些安全及可维护性的问题。请参照 <a href="http://scikit-learn.org/stable/modules/model_persistence.html#model-persistence" target="_blank" rel="external">持久模型</a> 一节了解sk-learn中关于此类的更多信息。</p>
<h1 id="常用惯例"><a href="#常用惯例" class="headerlink" title="常用惯例"></a>常用惯例</h1><p>sk-learn 估计量遵循一些确定的规则使其预测。</p>
<ul>
<li>类型转换<br>除其他特殊情况，输入类型为 <strong>float64</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn <span class="keyword">import</span> random_projection</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>rng = np.random.RandomState(<span class="number">0</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>X = rng.rand(<span class="number">10</span>, <span class="number">2000</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>X = np.array(X, dtype=<span class="string">'float32'</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>X.dtype</div><div class="line">dtype(<span class="string">'float32'</span>)</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>transformer = random_projection.GaussianRandomProjection()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>X_new = transformer.fit_transform(X)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>X_new.dtype</div><div class="line">dtype(<span class="string">'float64'</span>)</div></pre></td></tr></table></figure>
</li>
</ul>
<p>在此例中，X是<strong>float32</strong>，之后通过<strong>fit_transform(X)</strong>转化为<strong>float64</strong>。</p>
<p>回归分析目标被转化为<strong>float64</strong>，归类目标则使用如下方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>iris = datasets.load_iris()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf = SVC()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf.fit(iris.data, iris.target)  </div><div class="line">SVC(C=<span class="number">1.0</span>, cache_size=<span class="number">200</span>, class_weight=<span class="keyword">None</span>, coef0=<span class="number">0.0</span>,</div><div class="line">  decision_function_shape=<span class="keyword">None</span>, degree=<span class="number">3</span>, gamma=<span class="string">'auto'</span>, kernel=<span class="string">'rbf'</span>,</div><div class="line">  max_iter=<span class="number">-1</span>, probability=<span class="keyword">False</span>, random_state=<span class="keyword">None</span>, shrinking=<span class="keyword">True</span>,</div><div class="line">  tol=<span class="number">0.001</span>, verbose=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>list(clf.predict(iris.data[:<span class="number">3</span>]))</div><div class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf.fit(iris.data, iris.target_names[iris.target])  </div><div class="line">SVC(C=<span class="number">1.0</span>, cache_size=<span class="number">200</span>, class_weight=<span class="keyword">None</span>, coef0=<span class="number">0.0</span>,</div><div class="line">  decision_function_shape=<span class="keyword">None</span>, degree=<span class="number">3</span>, gamma=<span class="string">'auto'</span>, kernel=<span class="string">'rbf'</span>,</div><div class="line">  max_iter=<span class="number">-1</span>, probability=<span class="keyword">False</span>, random_state=<span class="keyword">None</span>, shrinking=<span class="keyword">True</span>,</div><div class="line">  tol=<span class="number">0.001</span>, verbose=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>list(clf.predict(iris.data[:<span class="number">3</span>]))  </div><div class="line">[<span class="string">'setosa'</span>, <span class="string">'setosa'</span>, <span class="string">'setosa'</span>]</div></pre></td></tr></table></figure></p>
<p>在这里，第一个<strong>predict()</strong>返回一个整型队列，因为<strong>iris.target</strong>(一个整型队列)被用在<em>fit</em>中。第二个<strong>predict</strong>返回一个字符串队列，因为iris.target_names被用于匹配。</p>
<ul>
<li>改装和升级参数</li>
</ul>
<p>模拟器的超参数可以在通过使用 <a href="http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline.set_params" target="_blank" rel="external">sklearn.pipeline.Pipeline.set_params</a> 方法建成后进行升级。多次使用fit()将覆盖之前fit()学习到的内容：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>rng = np.random.RandomState(<span class="number">0</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>X = rng.rand(<span class="number">100</span>, <span class="number">10</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y = rng.binomial(<span class="number">1</span>, <span class="number">0.5</span>, <span class="number">100</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>X_test = rng.rand(<span class="number">5</span>, <span class="number">10</span>)</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf = SVC()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf.set_params(kernel=<span class="string">'linear'</span>).fit(X, y)  </div><div class="line">SVC(C=<span class="number">1.0</span>, cache_size=<span class="number">200</span>, class_weight=<span class="keyword">None</span>, coef0=<span class="number">0.0</span>,</div><div class="line">  decision_function_shape=<span class="keyword">None</span>, degree=<span class="number">3</span>, gamma=<span class="string">'auto'</span>, kernel=<span class="string">'linear'</span>,</div><div class="line">  max_iter=<span class="number">-1</span>, probability=<span class="keyword">False</span>, random_state=<span class="keyword">None</span>, shrinking=<span class="keyword">True</span>,</div><div class="line">  tol=<span class="number">0.001</span>, verbose=<span class="keyword">False</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf.predict(X_test)</div><div class="line">array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>])</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf.set_params(kernel=<span class="string">'rbf'</span>).fit(X, y)  </div><div class="line">SVC(C=<span class="number">1.0</span>, cache_size=<span class="number">200</span>, class_weight=<span class="keyword">None</span>, coef0=<span class="number">0.0</span>,</div><div class="line">  decision_function_shape=<span class="keyword">None</span>, degree=<span class="number">3</span>, gamma=<span class="string">'auto'</span>, kernel=<span class="string">'rbf'</span>,</div><div class="line">  max_iter=<span class="number">-1</span>, probability=<span class="keyword">False</span>, random_state=<span class="keyword">None</span>, shrinking=<span class="keyword">True</span>,</div><div class="line">  tol=<span class="number">0.001</span>, verbose=<span class="keyword">False</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf.predict(X_test)</div><div class="line">array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>])</div></pre></td></tr></table></figure></p>
<p>在这里，默认kernal <strong>rbf</strong> 在模拟器通过<strong>SVC()</strong>建立之后一开始被改为 <strong>linear</strong>，之后被改回<strong>rbf</strong>以重新训练模拟器并做下次预测。</p>
<h1 id="未完待续，有空再更。"><a href="#未完待续，有空再更。" class="headerlink" title="未完待续，有空再更。"></a>未完待续，有空再更。</h1>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Voigtlander 50/2.8 dkl]]></title>
      <url>http://yoursite.com/2016/09/04/Voigtlander-50-2-8-dkl/</url>
      <content type="html"><![CDATA[<blockquote><p>Your first 10000 photographs are your worst.</p>
<footer><strong>Henri Cartier-Bresson</strong></footer></blockquote>
<a id="more"></a>
<p><strong>试片</strong></p>
<img src="/uploads/Voigtlander-50-2-8-dkl/DSC04053.jpg" class="full-image">
<img src="/uploads/Voigtlander-50-2-8-dkl/DSC06068.jpg" class="full-image">
<img src="/uploads/Voigtlander-50-2-8-dkl/DSC04231.jpg" class="full-image">]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Min.教你远离剧荒（2）]]></title>
      <url>http://yoursite.com/2016/09/03/Min-%E6%95%99%E4%BD%A0%E8%BF%9C%E7%A6%BB%E5%89%A7%E8%8D%92%EF%BC%882%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>上次我们已经获得了tag标签，现在我打算对标签进行排列组合。<br>这里仅计算组合结果，不进行排列，计算公式为：<br><img src="http://i1.piimg.com/567571/b82866d6d7abe928.png" alt=""></p>
<p>计算出所有tag在不同n情况下的组合，再用这些组合进行反向筛选电影，n越大权重越高，所以对n降序排列。<br>有些电影的tag数过多，所以决定只取5个tag。<br><a id="more"></a><br>先上代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="string">''' Mingo's movie_recommend script '''</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> itertools</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">arrange</span><span class="params">(tag)</span>:</span></div><div class="line">   <span class="keyword">for</span> num <span class="keyword">in</span> range(len(tag), <span class="number">0</span>, <span class="number">-1</span>):</div><div class="line">      s = (list(itertools.combinations(tag, num)))</div><div class="line">      print(s)</div></pre></td></tr></table></figure></p>
<p>这里我用到了itertools模块进行组合运算，itertools.combinations(tag, num)即从tag中取出num个元素进行组合，各组合return一个元组，list返回一个列表。这里range是反向排列的，因为num越大权重越大。输出如下：<br><img src="http://i1.piimg.com/567571/f0f362a7ca328610.png" alt=""></p>
<p>得到了标签的组合，现在再观察豆瓣电影标签查找电影的页面：<br><img src="http://i1.piimg.com/567571/c1e6043b0e01991c.png" alt=""></p>
<p>发现url的规律即：</p>
<pre><code>https://movie.douban.com/tag/
</code></pre><p>加上：<br>    东野圭吾%20推理%20日本%20福山雅治%20悬疑<br>其中的%20为空格，再对中文进行编码即可，于是修改代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">arrange</span><span class="params">(tag)</span>:</span></div><div class="line">   url_tag = <span class="string">"https://movie.douban.com/tag/"</span></div><div class="line">   <span class="keyword">for</span> num <span class="keyword">in</span> range(len(tag), <span class="number">0</span>, <span class="number">-1</span>):</div><div class="line">      s = (list(itertools.combinations(tag, num)))</div><div class="line">      <span class="comment">#print(s)</span></div><div class="line">      <span class="keyword">for</span> each <span class="keyword">in</span> s:</div><div class="line">         url = url_tag + urllib.parse.quote(<span class="string">' '</span>.join(each))</div><div class="line">         <span class="keyword">print</span> (url)</div></pre></td></tr></table></figure></p>
<p>输出了通过所有的tag组合查找的网址。<br><img src="http://i1.piimg.com/567571/659fd8254f470d3d.png" alt=""></p>
<p>查看输出url的网页源代码，很容易找到了电影名所在位置。<br><img src="http://i1.piimg.com/567571/f06b850f2a7a4f9d.png" alt=""><br>但也有的tag组合可能查找不到电影，则查找不到a class：<br><img src="http://i1.piimg.com/567571/e2a7c25647de8646.png" alt=""><br>查找不到的即跳过，选出查找到的前十部即可。</p>
<p>直接上代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="string">''' Mingo's movie_recommend script '''</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">arrange</span><span class="params">(tag,url_pre)</span>:</span></div><div class="line"></div><div class="line">   url_tag = <span class="string">"https://movie.douban.com/tag/"</span></div><div class="line">   all = []</div><div class="line"></div><div class="line">   <span class="keyword">for</span> num <span class="keyword">in</span> range(len(tag), <span class="number">0</span>, <span class="number">-1</span>):</div><div class="line">      s = (list(itertools.combinations(tag, num)))</div><div class="line">      <span class="comment">#print(s)</span></div><div class="line">      <span class="keyword">for</span> each <span class="keyword">in</span> s:</div><div class="line">         url = url_tag + urllib.parse.quote(<span class="string">' '</span>.join(each))</div><div class="line">         html = get_html(url)</div><div class="line">         end = <span class="number">0</span></div><div class="line">         <span class="comment">#print (url)</span></div><div class="line">         <span class="keyword">for</span> i <span class="keyword">in</span> range(len(re.findall(<span class="string">'a class="nbg" href="'</span>, html))):</div><div class="line">               start = html.find(<span class="string">'a class="nbg" href="'</span>, end) + len(<span class="string">'a class="nbg" href="'</span>)</div><div class="line">               end = html.find(<span class="string">'"  title='</span>, start)</div><div class="line">               name_start = end + len(<span class="string">'"  title="'</span>)</div><div class="line">               name_end = html.find(<span class="string">'"&gt;'</span>, name_start)</div><div class="line"></div><div class="line">               <span class="keyword">if</span> html[start:end] != url_pre <span class="keyword">and</span> <span class="keyword">not</span> re.findall(html[start:end], <span class="string">''</span>.join(all)):</div><div class="line">                  all.append(html[name_start:name_end] + <span class="string">"\t"</span> + html[start:end] + <span class="string">"\n"</span>)</div><div class="line">                  <span class="keyword">if</span> len(all) == <span class="number">10</span>:</div><div class="line">                     <span class="keyword">return</span> <span class="string">''</span>.join(all)</div><div class="line">               <span class="comment">#print (html[name_start:name_end] + "\n" + html[start:end])</span></div><div class="line">               <span class="comment">#print (start,end,name_start,name_end)</span></div></pre></td></tr></table></figure></p>
<p>这里写的有些繁琐，首先arrange函数传入2个参数，tag为之前的标签，url_pre 为输入电影的地址。创建了一个all的list用于储存数据，url获得了标签组合的网址，利用之前的get_html()函数进行解析。之后在网页中查找地址，这里并没有用到BeautifulSoup，只用了普通的find查找。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">re.findall(<span class="string">'a class="nbg" href="'</span>, html)</div></pre></td></tr></table></figure></p>
<p>利用正则表达式查找出html电影的个数，进行循环。<br>start, end, name_start, name_end为4个数值，代表电影名起止位置及电影url起止位置。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> html[start:end] != url_pre <span class="keyword">and</span> <span class="keyword">not</span> re.findall(html[start:end], <span class="string">''</span>.join(all)):</div><div class="line">    all.append(html[name_start:name_end] + <span class="string">"\t"</span> + html[start:end] + <span class="string">"\n"</span>)</div><div class="line">    <span class="keyword">if</span> len(all) == <span class="number">10</span>:</div><div class="line">        <span class="keyword">return</span> <span class="string">''</span>.join(all)</div></pre></td></tr></table></figure></p>
<p>这里进行条件判断，两个条件：（1）要求此刻查找出的电影url与输入的电影url不同，这里就用到了arrange函数中传入的url_pre；（2）要求此刻查找出的电影url与all列表中已添加的电影url不同，以免最终结果中出现相同的电影。满足这两个条件即可将此电影url及名称添加进list。<br>添加完后再进行条件判断，如果添加个数达到10个则停止循环，直接返回all。</p>
<p>大致如此，让我们测试一下：<br><img src="http://i1.piimg.com/567571/a1781162b83465c6.png" alt=""><br><img src="http://i1.piimg.com/567571/31aae76f881e76fd.png" alt=""><br><img src="http://i1.piimg.com/567571/ceb0c22fb4a1a0cb.png" alt=""><br><img src="http://i1.piimg.com/567571/30622e6f229f00e3.png" alt=""></p>
<p>噢。。似乎还可以的样子。对输入单部影片来说，似乎就到这里可以了。。</p>
<p>但做单部影片也太没挑战性了，于是我决定在修改一下，在输入多部影片时，依旧能够找出最相近的影片。<br>首先多部影片就会有多个tag标签，不同影片可能会有相同的标签或不同的标签，所以需要搜集标签进行排序。<br>所以：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">tag_all = &#123;&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">tag_sort</span><span class="params">(tag,tag_all)</span>:</span></div><div class="line">   <span class="keyword">for</span> each <span class="keyword">in</span> tag:</div><div class="line">      <span class="keyword">if</span> each <span class="keyword">not</span> <span class="keyword">in</span> tag_all.keys():</div><div class="line">         tag_all[each] = <span class="number">1</span></div><div class="line">      <span class="keyword">else</span>:</div><div class="line">         tag_all[each]+=<span class="number">1</span></div><div class="line">   <span class="keyword">return</span> tag_all</div></pre></td></tr></table></figure></p>
<p>tag_sort()函数传入2个参数，tag为之前得到的标签，tag_all即为我们新创建的dict字典，用来储存tag并进行计数；tag为一个list，for each 遍历tag，进行条件判断：如果这个标签不存在于tag_all的key中，则将此标签添加进去；如果，存在，则将tag_all中此标签的values 加一。遍历完后此函数return一个tag_all字典。</p>
<p>为了配合，我们将主函数也修改了一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">   name = input(<span class="string">"movie name:"</span>)</div><div class="line">   name_list = name.split(<span class="string">','</span>)</div><div class="line">   tag_all = &#123;&#125;</div><div class="line">   <span class="keyword">for</span> name <span class="keyword">in</span> name_list:</div><div class="line">      <span class="comment">#print (name)</span></div><div class="line">      url_pre = <span class="string">"https://movie.douban.com/subject_search?search_text="</span> + urllib.parse.quote(name)</div><div class="line">      html = get_html(url_pre)</div><div class="line">      movie_url = analyse(html)</div><div class="line">      html_movie = get_html(movie_url)</div><div class="line">      movie_tag = tag(html_movie)</div><div class="line">      tag_all = tag_sort(movie_tag,tag_all)</div><div class="line">   <span class="keyword">print</span> (tag_all)</div><div class="line">   <span class="comment">#print (arrange(movie_tag, movie_url))</span></div></pre></td></tr></table></figure></p>
<p>电影名之间以”,”分隔，输出：<br><img src="http://i1.piimg.com/567571/4925f1d7494f8079.png" alt=""></p>
<p>看到输出的字典中喜剧，美国，动画的values最大，所以我们对字典进行排列：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">tag_dict = sorted(tag_all.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>], reverse=<span class="keyword">True</span>)</div><div class="line">movie_tag = []</div><div class="line"><span class="keyword">for</span> ta <span class="keyword">in</span> tag_dict[:<span class="number">5</span>]:</div><div class="line">   movie_tag.append(ta[<span class="number">0</span>])</div><div class="line"><span class="keyword">print</span> (movie_tag)</div></pre></td></tr></table></figure></p>
<p>这里tag_all.items()得到(键，值)的列表，sorted方法，通过key这个参数，reverse=TRUE表示降序。</p>
<p>最后输出：<br><img src="http://i1.piimg.com/567571/2d65fa295d095724.png" alt=""></p>
<p>看来没问题，排列输出正确。<br>将输出的list再拼接回去，再输出：<br><img src="http://i1.piimg.com/567571/ed4e250b40c64b93.png" alt=""><br><img src="http://i1.piimg.com/567571/85631317d1e0919b.png" alt=""><br><img src="http://i1.piimg.com/567571/1859e3b67e82c571.png" alt=""><br>okay，就这样完成了！效果似乎还不错。<br>于是你可以输入各种你喜欢的剧，看看这个小脚本到底给不给力吧！</p>
<p>最后把源代码上传到了github上。<br><a href="https://github.com/sdws1983" target="_blank" rel="external">Mingo’s Github</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Min.教你远离剧荒（1）]]></title>
      <url>http://yoursite.com/2016/08/11/Min-%E6%95%99%E4%BD%A0%E8%BF%9C%E7%A6%BB%E5%89%A7%E8%8D%92%EF%BC%881%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>假期嘛，就应该在家里吹着空调吃着冰棍看着剧。但是，剧荒了该怎么办！<br>好吧，我感觉最近也是没剧可看，于是我想着能不能自己写个荐剧的小脚本。<br><a id="more"></a><br>打开豆瓣电影，随便找一部影片打开。<br><img src="http://i2.buimg.com/567571/521009a7a5af14d7.png" alt=""></p>
<p>每部影片的界面大致如此，看看有什么可以利用来进行筛选的元素。<br>我大致的思路如此：输入一些自己喜欢的影片，找到这些影片的相似之处，之后再筛选推荐出我可能喜欢的影片。<br>于是我觉得可以利用右下角的标签过滤出自己喜欢的影片之间的相同点。<br>右键查看网页源代码，ctrl+F，查找标签：<br><img src="http://i2.buimg.com/567571/02b348bd575d5406.png" alt=""><br>okay，很容易地找到了标签的位置。</p>
<p>让我们从头开始，首先根据名字查找电影。在豆瓣电影搜索电影名字，很容易发现豆瓣就是地址栏后面加了search。搜索后页面出现了很多电影的名单，找第一个的地址即可，也是右键查看源代码：<br><img src="http://i2.buimg.com/567571/3b86518f782ed182.png" alt=""><br>发现了电影的地址，这里我使用Python3 + BeautifulSoup4直接得到，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="string">''' Mingo's movie_recommend script '''</span></div><div class="line"></div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">import</span> urllib.request</div><div class="line"><span class="keyword">import</span> urllib.parse</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_html</span><span class="params">(url)</span>:</span></div><div class="line"></div><div class="line">   send_headers = &#123;</div><div class="line">      <span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.80 Safari/537.36'</span>,</div><div class="line">      <span class="string">'Accept'</span>:<span class="string">'*/*'</span>,</div><div class="line">      <span class="string">'Connection'</span>:<span class="string">'keep-alive'</span>,</div><div class="line">      <span class="string">'Host'</span>:<span class="string">'movie.douban.com'</span></div><div class="line">   &#125;</div><div class="line"></div><div class="line">   req = urllib.request.Request(url,headers = send_headers)</div><div class="line">   response = urllib.request.urlopen(req)</div><div class="line">   html = response.read().decode(<span class="string">'utf-8'</span>)</div><div class="line"></div><div class="line">   <span class="keyword">return</span> html</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">analyse</span><span class="params">(html)</span>:</span></div><div class="line">   soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</div><div class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> soup.find_all(<span class="string">'a'</span>):</div><div class="line">      <span class="keyword">try</span>:</div><div class="line">         content = i[<span class="string">'href'</span>]</div><div class="line">         <span class="keyword">if</span> <span class="string">'/subject/'</span> <span class="keyword">in</span> content:</div><div class="line">            <span class="keyword">print</span> (content)</div><div class="line">            <span class="keyword">break</span></div><div class="line">      <span class="keyword">except</span>:</div><div class="line">         <span class="keyword">pass</span></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">   name = input(<span class="string">"movie name:"</span>)</div><div class="line">   url = <span class="string">"https://movie.douban.com/subject_search?search_text="</span> + urllib.parse.quote(name)</div><div class="line">   html = get_html(url)</div><div class="line">   analyse(html)</div></pre></td></tr></table></figure>
<p>这里需要注意的是url中的中文处理，这里使用<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">urllib.parse.quote()</div></pre></td></tr></table></figure></p>
<p>进行编码，BeautifulSoup直接获得标签内容。<br>输出结果得到电影地址，即：</p>
<pre><code>https://movie.douban.com/subject/21817627/
</code></pre><p>再获得得到的地址网页内容，根据上面的步骤获取标签：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="string">''' Mingo's movie_recommend script '''</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">tag</span><span class="params">(html)</span>:</span></div><div class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</div><div class="line">    tag = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> soup.find_all(<span class="string">'a'</span>):</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            content = i[<span class="string">'href'</span>]</div><div class="line">            <span class="keyword">if</span> <span class="string">'/tag/'</span> <span class="keyword">in</span> content:</div><div class="line">                tag.append(i.string)</div><div class="line">        <span class="keyword">except</span>:</div><div class="line">            <span class="keyword">pass</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> tag[<span class="number">1</span>:]</div></pre></td></tr></table></figure></p>
<p>输入电影地址，返回一个所有标签的列表，这样我们得到了tag：<br><img src="http://i2.buimg.com/567571/e6e3604402cb30a6.png" alt=""></p>
<p>于是我们准备再对tag进行处理，搜集所有影片的tag，之后对所有tag进行排列组合，再使用tag组合后进行反向搜索得到推荐电影，大致就是这个思路。</p>
<p>今天太迟了打算放到明天再继续。。</p>
<p>未完待续。。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Rookie's Quant-trading(1)]]></title>
      <url>http://yoursite.com/2016/08/05/Rookie-s-Quant-trading-1/</url>
      <content type="html"><![CDATA[<p><img src="http://i1.piimg.com/567571/8e5864d2fdceb0af.jpg" alt=""><br>量化交易似乎是一个很高大上的名词，隔行如隔山，以我短浅的股票投机经历，我也只能边学习边探讨。<br>也是从Crossin这个公众号受到启发，又跟师兄聊了一些投机经历，这两天开始小试牛刀一下。<br><a id="more"></a><br>平台选取在国内的：<br>    <em>优矿</em><br>    <a href="http://uqer.io" target="_blank" rel="external">uqer.io</a></p>
<p>进入，注册，然后开始研究，新建策略，这时候就给出了一个比较简单的策略。<br><img src="http://i1.piimg.com/567571/809b577337751962.png" alt=""></p>
<p>参考Crossin公众号的一些基本操作，打算自己先简单的编写一个策略试试手。<br>正好之前某天和师兄一起去下地，我们互相交流并探讨了一下关于股票投机的经历和一些操作观念。。从实验室聊到了车上，又从车上说到了地里。。总的来说，虽然我和师兄的投机观念有较大的不同，但也是可以求同存异的。<br>当天在地里，师兄就给我谈了谈他的一种交易手法。<br>基本是这样的：超短线进出，日跌幅小于3%，半仓；日跌幅大于3%小于6%，全仓；日跌幅大于6%，割肉止损；日涨幅大于9%，止盈。大概就是这些。<br>于是我试着把这个策略写了一下：<br><img src="http://i1.piimg.com/567571/20a052daed11c355.png" alt=""><br>简单解释一下：<br>start&amp;end：起始和结束时间，这里我是选择了一个2年的时间段。<br>universe：股池，这里我选择了师兄最喜欢交易的东航。<br>freq：策略类型，这里我选择的是日间交易<br>refresh_rate：调仓频率，由于是超短交易，所以我设为了每日交易。<br>initialize：账户初始化，暂时没有用到。<br>handle_data(account)：每次调仓的交易策略，最为主要。<br>    account.get_attribute_history：获取股票历时数据，这里传入的第一个参数“closePrice”为收盘价，第二个参数为10，即获取最近10天内该股的收盘价格，return一个list。<br>    account.universe：从全局变量universe和当前持有的股票池中，剔除了当天停牌、退市和数据异常股票的股池。<br>    hist[s][-1]/hist[s][-2]-1：最近交易日的日涨跌幅<br>    order_pct(s,0.5)：交易股票使其价值为账户总资产的50%，这里即半仓。<br>    order_to(s,0)：交易一定股票使交易后该股数量为0，即清仓。</p>
<p>大体就是如此，按此策略，我们试着跑了一下：<br><img src="http://i1.piimg.com/567571/24dfe884b4a7fc3a.png" alt=""><br>似乎还不错的样子，阿尔法达到了32.9%，应该说是轻松超过了基准，最大回撤还是有些大，达到了45.6%，不太理想，说明风险控制不足。于是我准备在此基础上再简单加一些风控机制。<br>即5日内跌幅大于10%或15日内涨幅超过50%时，全部清仓。<br>于是变成了：<br><img src="http://i1.piimg.com/567571/21c8e37d95511ce5.png" alt=""><br>再试着跑了一下：<br><img src="http://i1.piimg.com/567571/90ec267cb6bbe338.png" alt=""><br>看起来有效果了，阿尔法达到了62.9%，回撤也下降到了31.1%。从两图对比也可以看出在后半段15年8月左右的下跌中较好的控制了风险。</p>
<p>但我们也不能仅仅满足于此，股池内股票单一，只有一只东航，东航在14、15年涨幅巨大，本身就是只牛股，可能不太具有说服力，于是我又调仓换股，换个大蓝（烂）筹（丑），工商银行试试看。<br><img src="http://i1.piimg.com/567571/2ef731c4b539ddd0.png" alt=""><br>显然跟之前相比不尽如人意，想了想会不会是由于工行盘子太大，波动小导致此策略施展不开：）于是再换一只同样稳的不行的601857中国神油看看：<br><img src="http://i1.piimg.com/567571/132653e67386aeed.png" alt=""><br>果然也是不分上下，烂的抠脚啊。<br>既然如此，换成牛市后期和股灾中十分抢眼的国企改革龙头中粮生化看看：<br><img src="http://i1.piimg.com/567571/b3a14606ae444786.png" alt=""><br>嗯。。很惊人，再细看中粮这段时间的独立走势：<br><img src="http://i1.piimg.com/567571/90b4538335bf88bb.png" alt=""><br>发现策略在头肩顶结构后还能创出新高，嗯。。看来我做的这个简单风险控制策略还不错。。<br>当然我这只是粗略的写了一个小策略，也有许多不完善的地方，仓位的分散及股池内股票筛选、权重，都还没有制定，只是以一个简单的交易策略为依据，就当作练练手了。</p>
<p>可以参考：</p>
<p>阅读原文：Crossin的编程教室</p>
<p>优矿的API文档：<a href="https://uqer.io/help/faqApi/" target="_blank" rel="external">https://uqer.io/help/faqApi/</a></p>
<p>&amp;<br><img src="http://i1.piimg.com/567571/624d72ed1a888cdd.png" alt=""><br>讲了一些python做数据处理，之前有用R语言处理一些数据，虽然支持生信分析的包很多，但感觉用起来还是不太顺手，所以再准备学习一个。能用在平时学习上，顺便与量化交易做对接。</p>
<p>关于Quant-trading，我也只能闲暇时间学习一个，作为一个Rookie，我也只能偶尔写写心得，做一些微小的事情。</p>
]]></content>
    </entry>
    
  
  
</search>
